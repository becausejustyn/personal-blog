{
  "hash": "c6db2c5ba2ea00ea720f8b8b7d375e7a",
  "result": {
    "markdown": "---\ntitle: Implementing Restricted Cubic Splines in Python with scikit-learn\ndate: '2025-10-25'\ncategories:\n  - python\n  - machine-learning\n  - regression\n  - splines\ndescription: A brief description of your post\nbibliography: references.bib\n---\n\n## Introduction\n\nWhen modeling non-linear relationships in regression problems, we often face a trade-off between flexibility and interpretability. Polynomial regression can capture non-linearity but often exhibits unstable behavior at the boundaries. Restricted cubic splines (RCS) offer an elegant solution that provides flexibility in the middle of the data range while maintaining linear behavior at the extremes.\n\nIn this post, we'll implement a scikit-learn compatible transformer for restricted cubic splines and demonstrate its use in a practical pipeline.\n\n## What Are Restricted Cubic Splines?\n\nRestricted cubic splines, also known as natural cubic splines, are piecewise cubic polynomials that:\n\n1. **Join smoothly** at predetermined knot locations\n2. **Are linear** beyond the boundary knots (hence \"restricted\")\n3. **Have continuous** first and second derivatives at all knots\n\nThis makes them particularly useful for modeling relationships that may be non-linear in the middle range but shouldn't extrapolate wildly beyond the observed data range.\n\n::: {.callout-note}\n## Why Use Restricted Cubic Splines?\n\n- **Stable extrapolation**: Linear behavior at extremes prevents unrealistic predictions\n- **Fewer parameters**: Typically require fewer degrees of freedom than polynomial regression\n- **Interpretability**: Widely used in medical and epidemiological research [@harrell2015]\n- **Smooth curves**: No discontinuities or kinks in the fitted function\n:::\n\n## The Implementation\n\nOur implementation follows the approach described in @harrell2015, using quantile-based knot placement that has been validated across many applications.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\ndef linear_spline(x):\n    \"\"\"Positive part function: max(0, x)\"\"\"\n    xx = x.copy()\n    xx[xx < 0] = 0\n    return xx\n    \n    \nclass RestrictedCubicSpline(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Restricted Cubic Spline transformer for scikit-learn pipelines.\n    \n    Parameters\n    ----------\n    k : int, default=3\n        Number of knots. Must be between 3 and 7.\n        \n    Attributes\n    ----------\n    t_ : ndarray\n        Knot locations computed from training data.\n        \n    References\n    ----------\n    Harrell, F. E. (2015). Regression Modeling Strategies (2nd ed.). \n    Springer.\n    \"\"\"\n    \n    def __init__(self, k=3):\n        self.k = k\n    \n    def fit(self, X, y=None):\n        \"\"\"\n        Compute knot locations based on quantiles of X.\n        \n        Parameters\n        ----------\n        X : array-like of shape (n_samples, 1)\n            Training data.\n        y : Ignored\n            Not used, present for API consistency.\n            \n        Returns\n        -------\n        self : object\n            Fitted transformer.\n        \"\"\"\n        if self.k < 0:\n            return self\n        \n        # Convert to numpy array and ensure 2D\n        X = np.asarray(X)\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n               \n        # Knot locations from Regression Modeling Strategies\n        knot_locations = {\n            3: [0.1, 0.5, 0.9],\n            4: [0.05, 0.365, 0.65, 0.95],\n            5: [0.05, 0.275, 0.5, 0.725, 0.95],\n            6: [0.05, 0.23, 0.41, 0.59, 0.77, 0.95],\n            7: [0.025, 0.1833, 0.3417, 0.5, 0.6583, 0.8167, 0.975]\n        }\n        \n        if self.k > 7 or self.k < 3:\n            raise ValueError(\n                'Value of k is not supported. Set k between 3 and 7.'\n            )\n            \n        self._quantiles = knot_locations[self.k]\n        self.t_ = np.quantile(X, q=self._quantiles)\n        \n        return self\n    \n    def transform(self, X, y=None):\n        \"\"\"\n        Transform X into RCS basis expansion.\n        \n        Parameters\n        ----------\n        X : array-like of shape (n_samples, 1)\n            Data to transform.\n        y : Ignored\n            Not used, present for API consistency.\n            \n        Returns\n        -------\n        basis_expansion : ndarray of shape (n_samples, k-1)\n            Transformed features.\n        \"\"\"\n        # Convert to numpy array and ensure 2D\n        X = np.asarray(X)\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n        \n        n_observations = X.shape[0]\n        \n        if self.k < 0:\n            return X\n                \n        basis_expansion = np.zeros((n_observations, self.k - 1))\n        \n        # First basis function is just X\n        basis_expansion[:, 0] = X.ravel()\n        \n        # Compute remaining basis functions\n        for j in range(self.k - 2):\n            basis_function = 0\n            basis_function += linear_spline(X - self.t_[j])**3\n            basis_function -= (\n                linear_spline(X - self.t_[self.k - 2])**3 * \n                (self.t_[-1] - self.t_[j]) / (self.t_[-1] - self.t_[-2])\n            )\n            basis_function += (\n                linear_spline(X - self.t_[-1])**3 * \n                (self.t_[-2] - self.t_[j]) / (self.t_[-1] - self.t_[-2])\n            )\n            \n            # Normalize by range\n            basis_expansion[:, j + 1] = (\n                basis_function.ravel() / (self.t_[-1] - self.t_[0])**2\n            )\n            \n        return basis_expansion\n```\n:::\n\n\n## Practical Example: Modeling Age Effects\n\nLet's demonstrate RCS with a dataset where we'd expect non-linear relationships: predicting insurance charges based on age and other factors.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport polars as pl\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Generate synthetic insurance data\nnp.random.seed(42)\nn_samples = 1000\n\nage = np.random.uniform(18, 65, n_samples)\nbmi = np.random.normal(30, 6, n_samples)\nsmoker = np.random.choice(['yes', 'no'], n_samples, p=[0.2, 0.8])\n\n# Non-linear relationship with age\ncharges = (\n    250 * age + \n    5 * age**2 - \n    0.03 * age**3 + \n    400 * bmi + \n    20000 * (smoker == 'yes') + \n    np.random.normal(0, 3000, n_samples)\n)\n\ndf = pl.DataFrame({\n    'age': age,\n    'bmi': bmi,\n    'smoker': smoker,\n    'charges': charges\n})\n\nprint(df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nshape: (5, 4)\n┌───────────┬───────────┬────────┬──────────────┐\n│ age       ┆ bmi       ┆ smoker ┆ charges      │\n│ ---       ┆ ---       ┆ ---    ┆ ---          │\n│ f64       ┆ f64       ┆ str    ┆ f64          │\n╞═══════════╪═══════════╪════════╪══════════════╡\n│ 35.603386 ┆ 31.066206 ┆ no     ┆ 30023.721844 │\n│ 62.683572 ┆ 21.987934 ┆ no     ┆ 35351.367255 │\n│ 52.403715 ┆ 32.281187 ┆ no     ┆ 35298.427487 │\n│ 46.136949 ┆ 33.663514 ┆ yes    ┆ 52870.563142 │\n│ 25.332876 ┆ 33.358743 ┆ no     ┆ 24943.057103 │\n└───────────┴───────────┴────────┴──────────────┘\n```\n:::\n:::\n\n\n### Building the Pipeline\n\nNow let's create a workflow that uses RCS for age, standard scaling for BMI, and one-hot encoding for categorical variables. Since we're using Polars and want to avoid pandas, we'll manually handle the transformations.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Split the data - convert numeric and categorical separately\nX_age = df['age'].to_numpy().reshape(-1, 1)\nX_bmi = df['bmi'].to_numpy().reshape(-1, 1)\nX_smoker = df['smoker'].to_numpy().reshape(-1, 1)\ny = df['charges'].to_numpy()\n\n# Split into train/test\nindices = np.arange(len(df))\ntrain_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n\nX_age_train, X_age_test = X_age[train_idx], X_age[test_idx]\nX_bmi_train, X_bmi_test = X_bmi[train_idx], X_bmi[test_idx]\nX_smoker_train, X_smoker_test = X_smoker[train_idx], X_smoker[test_idx]\ny_train, y_test = y[train_idx], y[test_idx]\n\n# Create individual transformers\nage_transformer = RestrictedCubicSpline(k=5)\nbmi_transformer = StandardScaler()\nsmoker_transformer = OneHotEncoder(drop='first', sparse_output=False)\n\n# Fit transformers\nX_age_train_transformed = age_transformer.fit_transform(X_age_train)\nX_bmi_train_transformed = bmi_transformer.fit_transform(X_bmi_train)\nX_smoker_train_transformed = smoker_transformer.fit_transform(X_smoker_train)\n\n# Combine features\nX_train_combined = np.hstack([\n    X_age_train_transformed,\n    X_bmi_train_transformed,\n    X_smoker_train_transformed\n])\n\n# Transform test set\nX_age_test_transformed = age_transformer.transform(X_age_test)\nX_bmi_test_transformed = bmi_transformer.transform(X_bmi_test)\nX_smoker_test_transformed = smoker_transformer.transform(X_smoker_test)\n\nX_test_combined = np.hstack([\n    X_age_test_transformed,\n    X_bmi_test_transformed,\n    X_smoker_test_transformed\n])\n\n# Fit Ridge regression\nregressor = Ridge(alpha=1.0)\nregressor.fit(X_train_combined, y_train)\n\n# Evaluate\ny_pred = regressor.predict(X_test_combined)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"RMSE: ${rmse:,.2f}\")\nprint(f\"R² Score: {r2:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE: $2,825.51\nR² Score: 0.932\n```\n:::\n:::\n\n\n### Visualizing the Age Effect\n\nLet's visualize the learned non-linear relationship between age and charges:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Create prediction data\nage_range = np.linspace(18, 65, 100).reshape(-1, 1)\nbmi_constant = np.full((100, 1), 30)\nsmoker_constant = np.full((100, 1), 'no')\n\n# Transform prediction features\nage_pred_transformed = age_transformer.transform(age_range)\nbmi_pred_transformed = bmi_transformer.transform(bmi_constant)\nsmoker_pred_transformed = smoker_transformer.transform(smoker_constant)\n\nX_pred_combined = np.hstack([\n    age_pred_transformed,\n    bmi_pred_transformed,\n    smoker_pred_transformed\n])\n\n# Get predictions\npredictions = regressor.predict(X_pred_combined)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.scatter(X_age_train.ravel(), y_train, alpha=0.3, label='Training data')\nplt.plot(age_range.ravel(), predictions, 'r-', linewidth=2, \n         label='RCS fit (k=5)')\nplt.xlabel('Age (years)', fontsize=12)\nplt.ylabel('Insurance Charges ($)', fontsize=12)\nplt.title('Non-linear Age Effect Captured by Restricted Cubic Splines', \n          fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Fitted relationship between age and insurance charges using 5-knot RCS](post_files/figure-html/fig-age-effect-output-1.png){#fig-age-effect}\n:::\n:::\n\n\n### Comparing Different Numbers of Knots\n\n::: {.panel-tabset}\n\n## Cross-validation Comparison\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Compare different numbers of knots\nknot_options = [3, 4, 5, 6, 7]\ncv_scores = []\n\nfor k in knot_options:\n    # Create transformers for this k\n    age_trans_k = RestrictedCubicSpline(k=k)\n    bmi_trans_k = StandardScaler()\n    smoker_trans_k = OneHotEncoder(drop='first', sparse_output=False)\n    \n    # Fit and transform\n    X_age_trans = age_trans_k.fit_transform(X_age_train)\n    X_bmi_trans = bmi_trans_k.fit_transform(X_bmi_train)\n    X_smoker_trans = smoker_trans_k.fit_transform(X_smoker_train)\n    \n    X_combined = np.hstack([X_age_trans, X_bmi_trans, X_smoker_trans])\n    \n    # Cross-validation\n    reg_k = Ridge(alpha=1.0)\n    scores = cross_val_score(\n        reg_k, X_combined, y_train, \n        cv=5, scoring='neg_root_mean_squared_error'\n    )\n    cv_scores.append(-scores.mean())\n    \nresults_df = pl.DataFrame({\n    'Knots': knot_options,\n    'CV RMSE': cv_scores\n})\n\nprint(results_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nshape: (5, 2)\n┌───────┬─────────────┐\n│ Knots ┆ CV RMSE     │\n│ ---   ┆ ---         │\n│ i64   ┆ f64         │\n╞═══════╪═════════════╡\n│ 3     ┆ 3125.510915 │\n│ 4     ┆ 3133.936641 │\n│ 5     ┆ 3134.665595 │\n│ 6     ┆ 3136.286529 │\n│ 7     ┆ 3136.791526 │\n└───────┴─────────────┘\n```\n:::\n:::\n\n\n## Visualization\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 5))\nplt.plot(knot_options, cv_scores, 'o-', linewidth=2, markersize=8)\nplt.xlabel('Number of Knots', fontsize=12)\nplt.ylabel('Cross-validation RMSE ($)', fontsize=12)\nplt.title('Model Performance vs Number of Knots', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Cross-validation RMSE for different numbers of knots](post_files/figure-html/fig-knot-comparison-output-1.png){#fig-knot-comparison}\n:::\n:::\n\n\n:::\n\n## Key Considerations\n\n::: {.callout-tip}\n## Choosing the Number of Knots\n\n- **3 knots**: Simple, captures basic non-linearity\n- **4-5 knots**: Good default for most applications [@harrell2015]\n- **6-7 knots**: Use when you have large sample sizes (n > 500) and expect complex relationships\n\nRule of thumb: Start with 4-5 knots and use cross-validation to optimize.\n:::\n\n::: {.callout-warning}\n## Limitations\n\n- Requires sufficient data density across the range\n- Knot placement based on quantiles assumes reasonable distribution\n- May not capture discontinuities or sharp kinks\n- For variables with extreme outliers, consider winsorizing first\n:::\n\n## Conclusion\n\nRestricted cubic splines provide a principled way to model non-linear relationships while maintaining interpretability and stability. The scikit-learn compatible implementation shown here integrates seamlessly into standard ML pipelines, making it easy to incorporate RCS into your workflow.\n\nThe key advantages over polynomial regression are:\n\n1. Linear extrapolation beyond boundary knots\n2. Fewer degrees of freedom needed\n3. Better numerical stability\n4. Widely accepted in scientific literature\n\nTry incorporating RCS into your next regression problem where you suspect non-linear relationships!\n\n## References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "post_files"
    ],
    "filters": [],
    "includes": {}
  }
}