[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Justyn Rodrigues",
    "section": "",
    "text": "Hello!\n\n\n\n\n\n\nJust keep in mind\n\n\n\nAs I have started learning how to make blogs and the front-end side of it, I have made many errors with my blog so there might be a few things broken at the moment. If you notice an problem, feel free to create an issue on the  repo. Thanks!\n\n\n ORCID"
  },
  {
    "objectID": "posts/barplots/post.html",
    "href": "posts/barplots/post.html",
    "title": "Barplots",
    "section": "",
    "text": "Note\n\n\n\nI have currently commented out my custom theme since I have not set up custom fonts yet on this computer.\n\n\nI have been planning on writing a post for a few weeks, but I keep changing my mind on what I want to write about. I have been going through my Github profile lately and tidying it up. It has made me realise how much code I had stashed away that I did not need, so I am trying to get into the habit of only keeping code that is available on Github to keep organised and accountable.\nI have also been doing a lot of NFL data viz, which I found a lot harder than I thought it would be. I’ll share them at a later date, but I thought I would do a quick post at 11pm at some tricks I learnt with using text labels. I thought I would use the penguins dataset from palmerpenguins since it does not appear to be as common as others such as mtcars.\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n# personal package with plotting theme. \n# I wrote it at the start of 2021, so it might have some issues\n#library(becausejustynfun) \n\npenguins &lt;- palmerpenguins::penguins\n\nEven though I am using the cleaned version, I find it good practice to use glimpse() just to be safe.\n\npenguins |&gt; glimpse()\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nNext, I am checking for missing values.\n\n#penguins |&gt;\n#  summarise(across(everything(), ~sum(is.na(.))))\n\npenguins |&gt; map_dbl(~sum(is.na(.)))\n\n          species            island    bill_length_mm     bill_depth_mm \n                0                 0                 2                 2 \nflipper_length_mm       body_mass_g               sex              year \n                2                 2                11                 0 \n\n\nTo see how I will deal with them I like to see the unique values across each column.\n\n#penguins |&gt;\n#  summarise(across(everything(), n_distinct))\n\npenguins |&gt; \n  map_dbl(~n_distinct(.))\n\n          species            island    bill_length_mm     bill_depth_mm \n                3                 3               165                81 \nflipper_length_mm       body_mass_g               sex              year \n               56                95                 3                 3 \n\n\nI’ll have a glance at the missing values to see if they are important.\n\npenguins |&gt; \n  filter(if_any(everything(), is.na)) \n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           NA            NA                  NA          NA\n 2 Adelie  Torgersen           34.1          18.1               193        3475\n 3 Adelie  Torgersen           42            20.2               190        4250\n 4 Adelie  Torgersen           37.8          17.1               186        3300\n 5 Adelie  Torgersen           37.8          17.3               180        3700\n 6 Adelie  Dream               37.5          18.9               179        2975\n 7 Gentoo  Biscoe              44.5          14.3               216        4100\n 8 Gentoo  Biscoe              46.2          14.4               214        4650\n 9 Gentoo  Biscoe              47.3          13.8               216        4725\n10 Gentoo  Biscoe              44.5          15.7               217        4875\n11 Gentoo  Biscoe              NA            NA                  NA          NA\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nGiven the NA values were low, I will just drop them.\n\npenguins &lt;- penguins |&gt;\n  na.exclude()\n\n#to check\nmap_dfc(penguins, sum(is.na(penguins)))\n\n# A tibble: 0 × 0\n\n#map_df(penguins, .f = sum(is.na(penguins)))\n\nI am going to create a seperate df with the count values. You can easily just use add_count(), or just rely on ggplot2 to do that for you, but later on we are going to make more adjustments to the df that will make it easier if there is a mistake.\n\n#df of counts\npenguins_sum &lt;- penguins |&gt;\n  count(species, sort = TRUE) |&gt; \n  mutate(\n    species = fct_rev(fct_inorder(species)),\n    perc = paste0(sprintf(\"%4.1f\", n / sum(n) * 100), \"%\") # percentage label\n    )\n\n#scales alternative\n#mutate(perc = scales::percent(n / sum(n), accuracy = .1, trim = FALSE))\n\n\npenguins_sum |&gt;\nggplot(aes(x = n, y = species)) +\n  geom_col(fill = \"gray70\") +\n  ## add percentage labels\n  geom_text(aes(label = perc)) \n\n\n\n\n\n## prepare non-aggregated data set with lumped and ordered factors\npenguins_fct &lt;- penguins |&gt; \n  dplyr::mutate(\n    total = dplyr::n(),\n    species = stringr::str_to_title(species),\n    species = forcats::fct_rev(forcats::fct_infreq(species))\n  )\n\npenguins_sum &lt;- penguins_sum |&gt; \n  mutate(\n    colour = case_when(\n      row_number() == 1 ~ \"#468499\",\n      row_number() == 2 ~ \"#E697AC\",\n      row_number() == 3 ~ \"#81CDE6\",\n      ## all others should be gray\n      TRUE ~ \"gray70\"\n    )\n  )\n\nThis is one approach were we conditionally colour each variable of choice.\n\npenguins_sum |&gt;\n  ggplot(aes(\n    x = n, \n    y = species, \n    fill = colour)) +\n  geom_col() +\n  geom_text(\n    aes(label = perc),\n    hjust = 1, nudge_x = -.5\n  ) +\n  ## add custom colors\n  scale_fill_identity(guide = \"none\") \n\n\n\n\nWe also have a lot of control over the font used. There can be challenges when installing a font onto your system for the first time, but I might go through that another time since I struggled with it for quite a while. Maybe I am just a silly billy.\n\npenguins_sum |&gt;\nggplot(aes(\n  x = n, \n  y = species, \n  fill = colour)) +\n  geom_col() +\n  geom_text(\n    aes(label = perc), \n    hjust = 1, nudge_x = -.5,\n    size = 4, fontface = \"bold\"\n  ) +\n  ## reduce spacing between labels and bars\n  scale_x_continuous(expand = c(.01, .01)) +\n  scale_fill_identity(guide = \"none\") \n\n\n\n\nSometimes the colour of the font does not match well with the plot. This can be challenging when you have more than a few colours, so you might not want to manually adjust every single one. One option is to add white to the label with fill = \"white\".\n\npenguins_sum |&gt;\n  ggplot(aes(\n    x = n, \n    y = species, \n    fill = colour)) +\n  geom_col() +\n  geom_label(\n    aes(label = perc), \n    hjust = 0.95, nudge_x = -.5,\n    size = 4, fontface = \"bold\",\n    ## turn into white box without outline\n    fill = \"white\", label.size = 0\n  ) +\n  scale_x_continuous(expand = c(.01, .01)) +\n  scale_fill_identity(guide = \"none\") +\n  theme(\n    axis.text.y = element_text(size = 14, hjust = 1),\n    plot.margin = margin(rep(15, 4))\n  )\n\n\n\n\nLikewise, it is possible to adjust the position of the text labels conditionally. I think ggplot2 biggest challenge is learning what you can do.\n\npenguins_sum |&gt; \n  mutate(\n    ## set justification based on data \n    ## so that only the first label is placed inside\n    place = if_else(row_number() == 1, 1, 0),\n    ## add some spacing to labels since we cant use nudge_x anymore\n    perc = paste(\" \", perc, \" \")\n  ) |&gt; \n  ggplot(aes(\n    x = n, \n    y = species, \n    fill = colour)) +\n  geom_col() +\n  geom_text(\n    aes(label = perc, hjust = place), \n    fontface = \"bold\"\n  ) +\n  scale_x_continuous(expand = c(.01, .01)) +\n  scale_fill_identity(guide = \"none\") +\n  theme(\n    plot.margin = margin(rep(15, 4))\n  )\n\n\n\n\nYou can use different positions of text labels to highlight things of interest, or sometimes adjusting it makes it easier to work around other features of the plot such as the background or legend.\nThat is all for today. I’ll try to post next week about more data viz.\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.2      forcats_0.5.1        stringr_1.5.0       \n [4] dplyr_1.1.1          purrr_1.0.1          readr_2.1.4         \n [7] tidyr_1.3.0          tibble_3.2.1         ggplot2_3.4.1       \n[10] tidyverse_2.0.0      palmerpenguins_0.1.1\n\nloaded via a namespace (and not attached):\n [1] compiler_4.1.2    pillar_1.9.0      tools_4.1.2       digest_0.6.31    \n [5] timechange_0.2.0  jsonlite_1.8.4    evaluate_0.20     lifecycle_1.0.3  \n [9] gtable_0.3.3      pkgconfig_2.0.3   rlang_1.1.0       cli_3.6.1        \n[13] rstudioapi_0.14   yaml_2.3.7        xfun_0.37         fastmap_1.1.1    \n[17] withr_2.5.0       knitr_1.42        hms_1.1.3         generics_0.1.3   \n[21] vctrs_0.6.1       htmlwidgets_1.6.2 grid_4.1.2        tidyselect_1.2.0 \n[25] glue_1.6.2        R6_2.5.1          fansi_1.0.4       rmarkdown_2.20   \n[29] farver_2.1.1      tzdb_0.3.0        magrittr_2.0.3    scales_1.2.1     \n[33] htmltools_0.5.5   colorspace_2.1-0  labeling_0.4.2    utf8_1.2.3       \n[37] stringi_1.7.12    munsell_0.5.0"
  },
  {
    "objectID": "posts/ggplot_colour_text/post.html",
    "href": "posts/ggplot_colour_text/post.html",
    "title": "Colouring Text in ggplot2",
    "section": "",
    "text": "One of the more underrated options in data viz is to colour text. You can use this to highlight important information, or to improve how the viz looks. It is quite simple with ggtext, so I will not go into too much detail.\n\nlibrary(tidyverse)\nlibrary(ggtext) \nlibrary(grid)\nlibrary(glue)\n\nI originally wrote this post at 10pm before bed, so I used mtcars to allow myself to get this done quickly. To have the colour applied to the text, you need to specify element_markdown() in the theme() function.\n\nmtcars |&gt;\n  ggplot(aes(\n    x = wt, \n    y = mpg, \n    colour = factor(cyl)\n  )) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Set2\") +\n  labs(\n    x = \"&lt;span style = 'color:#93C1DE'&gt;**wt**&lt;/span&gt;\",\n    y = \"&lt;span style = 'color:#2ca25f'&gt;**mpg**&lt;/span&gt;\",\n    title = \"Lower &lt;span style='color:#93C1DE'&gt;**wt**&lt;/span&gt; tends to increase the amount of &lt;span style='color:#2ca25f'&gt;**mpg**&lt;/span&gt;\",\n    colour = \"\"\n  ) +\n  theme(\n    plot.title = element_markdown(),\n    axis.title.x = element_markdown(),\n    axis.title.y = element_markdown()\n  )\n\n\n\n\nLike I mentioned earlier, you can highlight a particular feature of interest by colouring a particular column or data point, then matching that colour in the title. To only have one of the columns coloured, you need to do a little bit more work, but it is still quite simple.\n\niris |&gt;\n  group_by(Species) |&gt;\n  summarise(mean_petal_width = mean(Petal.Width), .groups = \"drop\") |&gt;\n  mutate(\n    colour = c(\"lightgray\", \"lightgray\", \"#0072B2\"), \n    name = glue(\"&lt;i style='color:{colour}'&gt;{Species}&lt;/i&gt;\"),\n    name = fct_reorder(name, mean_petal_width)\n  ) |&gt;\n  ggplot(aes(\n    x = name,\n    y = mean_petal_width,\n    fill = colour\n  )) +\n  geom_col() +\n  hrbrthemes::theme_ipsum() +\n  scale_fill_identity() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"&lt;span style = 'color: #43a2ca;'&gt;Virginica irises&lt;/span&gt; have the largest average sepal width\"\n  ) +\n  theme(\n    plot.title = element_markdown(),\n    axis.text.x = element_markdown(),\n    legend.position = \"none\"\n  )\n\n\n\n\nLastly, to have all the columns coloured, you essentially do the same thing. You just need to make sure that the colour column is a factor, and that the name column is ordered.\n\niris |&gt;\n  group_by(Species) |&gt;\n  summarise(mean_petal_width = mean(Petal.Width), .groups = \"drop\") |&gt;\n  mutate(\n    colour = c(\"#91529e\", \"#009E73\", \"#0072B2\"), #009E73\n    name = glue(\"&lt;i style='color:{colour}'&gt;{Species}&lt;/i&gt;\"),\n    name = fct_reorder(name, mean_petal_width)\n  ) |&gt;\n  ggplot(aes(\n    x = name,\n    y = mean_petal_width,\n    fill = colour\n  )) +\n  geom_col() +\n  hrbrthemes::theme_ipsum() +\n  scale_fill_identity() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"&lt;span style = 'color: #43a2ca;'&gt;Virginica irises&lt;/span&gt; have the largest average sepal width\"\n  ) +\n  theme(\n    plot.title = element_markdown(),\n    axis.text.x = element_markdown(),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] glue_1.6.2      ggtext_0.1.2    lubridate_1.9.2 forcats_0.5.1  \n [5] stringr_1.5.0   dplyr_1.1.1     purrr_1.0.1     readr_2.1.4    \n [9] tidyr_1.3.0     tibble_3.2.1    ggplot2_3.4.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.10             digest_0.6.31           utf8_1.2.3             \n [4] mime_0.12               R6_2.5.1                evaluate_0.20          \n [7] pillar_1.9.0            gdtools_0.3.2           rlang_1.1.0            \n[10] hrbrthemes_0.8.6        curl_5.0.0              rstudioapi_0.14        \n[13] extrafontdb_1.0         rmarkdown_2.20          labeling_0.4.2         \n[16] extrafont_0.19          htmlwidgets_1.6.2       munsell_0.5.0          \n[19] gridtext_0.1.5          shiny_1.7.4             httpuv_1.6.9           \n[22] compiler_4.1.2          xfun_0.37               systemfonts_1.0.4      \n[25] pkgconfig_2.0.3         gfonts_0.2.0            htmltools_0.5.5        \n[28] tidyselect_1.2.0        fontBitstreamVera_0.1.1 httpcode_0.3.0         \n[31] fansi_1.0.4             later_1.3.0             crayon_1.5.2           \n[34] tzdb_0.3.0              withr_2.5.0             commonmark_1.9.0       \n[37] crul_1.3                xtable_1.8-4            jsonlite_1.8.4         \n[40] Rttf2pt1_1.3.12         gtable_0.3.3            lifecycle_1.0.3        \n[43] magrittr_2.0.3          scales_1.2.1            cachem_1.0.7           \n[46] cli_3.6.1               stringi_1.7.12          farver_2.1.1           \n[49] promises_1.2.0.1        xml2_1.3.3              ellipsis_0.3.2         \n[52] generics_0.1.3          vctrs_0.6.1             RColorBrewer_1.1-3     \n[55] tools_4.1.2             markdown_1.5            fontquiver_0.2.1       \n[58] hms_1.1.3               fastmap_1.1.1           yaml_2.3.7             \n[61] timechange_0.2.0        colorspace_2.1-0        fontLiberation_0.1.0   \n[64] memoise_2.0.1           knitr_1.42"
  },
  {
    "objectID": "posts/code_snippets/post.html",
    "href": "posts/code_snippets/post.html",
    "title": "Code Snippets",
    "section": "",
    "text": "This is a post where I have some snippets of code that I find useful, but did not want to dedicate an entire post to them."
  },
  {
    "objectID": "posts/code_snippets/post.html#create-folder-and-file-inside",
    "href": "posts/code_snippets/post.html#create-folder-and-file-inside",
    "title": "Code Snippets",
    "section": "Create Folder and File Inside",
    "text": "Create Folder and File Inside\nSay you want to create a new folder and then create an item inside that newly created folder. This is the line I use when creating a new directory for my blog. First this creates a new folder with mkdir, then touch allows me to create a new file type in the specified path. touch can be used for any file type, so this becomes quite useful for other tasks too.\nmkdir -p posts/code_snippets && touch posts/code_snippets/post.qmd"
  },
  {
    "objectID": "posts/code_snippets/post.html#create-folders-w-looping",
    "href": "posts/code_snippets/post.html#create-folders-w-looping",
    "title": "Code Snippets",
    "section": "Create Folders w/ Looping",
    "text": "Create Folders w/ Looping\nAt the start of every semester I liked to make a folder for each week. Instead of manually created a folder for each week, you can use a for loop using the command line, e.g. \nfor i in {1..10}; do mkdir \"Week $i\"; done\nWill make 10 folders from Week 1 to Week 10. Changing the values or name of the string obviously can change this.\nLikewise, in python or r:\n\nRPython\n\n\nfor (i in 1:10) {\n  dir.create(paste('Week', i))\n}\n\n\nimport os\n\nfor i in range(1, 11):\n    os.mkdir(f'Week {i}')"
  },
  {
    "objectID": "posts/code_snippets/post.html#resizing-images",
    "href": "posts/code_snippets/post.html#resizing-images",
    "title": "Code Snippets",
    "section": "Resizing Images",
    "text": "Resizing Images\nIn this example I had a dataset of 100,000 images, which was 32GB. It was large for what I wanted to do, so I wrote a script to resize all the images and save them into a new folder.\nI tested it on 100 images, which took 5 seconds to run and the folder size went from 30.5MB to 8.7MB after I resized the images from 512 by 512 to 256 by 256.\n\n\nThis can also be done with the command line and R, but most of the times I want to resize images I am already using python, so there is no need to complicate the task.\nimport os\nfrom PIL import Image\n\nnew_width, new_height = 256, 256\n\n# location of the images\nsource_folder = 'dataset_100/'\n# folder where we want to save the resized images\ndestination_folder = 'resized_256/'\n# creates a list of every file in the source folder\ndirectory = os.listdir(source_folder)\n\n# for every image in the source folder    \nfor item in directory:\n    # if the file does not end with _seg & is a jpg file\n    if item.endswith(\".png\") and '_seg' not in item:\n        # open the image and assign it to the variable img\n        img = Image.open(source_folder + item)\n        # resize the image using the above dimensions\n        imgResize = img.resize((new_width, new_height))\n        # save the resized image to the destination folder\n        imgResize.save(destination_folder + item[:-4] +'.png', quality = 90)\nYou can also do this with command line, but at this point, I do not find it quicker.\nfind dataset_100 -type f -name '*.png' ! -name '*_seg*' -exec bash -c 'mkdir -p resized_256; convert \"{}\" -resize 256x256 \"resized_256/$(basename \"{}\" .png)_resized.png\"' \\;"
  },
  {
    "objectID": "posts/combinations/post.html",
    "href": "posts/combinations/post.html",
    "title": "Combinations-vs-Permutation",
    "section": "",
    "text": "Note\n\n\n\nI do not remember why I wrote this post back in 2021."
  },
  {
    "objectID": "posts/combinations/post.html#possible-variations",
    "href": "posts/combinations/post.html#possible-variations",
    "title": "Combinations-vs-Permutation",
    "section": "Possible Variations",
    "text": "Possible Variations\n\nWhen the order doesn’t matter, it is a combination;\nWhen the order does matter it is a permutation."
  },
  {
    "objectID": "posts/combinations/post.html#repetition",
    "href": "posts/combinations/post.html#repetition",
    "title": "Combinations-vs-Permutation",
    "section": "Repetition",
    "text": "Repetition\n\nRepetition could be allowed in a situation where you wanted to know the character in a password (you can have multiple letters or numbers);\nNo repetition is when there is no replacement in the values such as lotto numbers or selecting a seat (you cannot sit where somebody is already sitting)."
  },
  {
    "objectID": "posts/combinations/post.html#example",
    "href": "posts/combinations/post.html#example",
    "title": "Combinations-vs-Permutation",
    "section": "Example",
    "text": "Example\nI had a gift card where two of the numbers were scratched off as I peeled back the scratchy thing. I did not want to contact the store if I could avoid that.\nFor example,\nA3E# - 3JH# - 45HL\nwhere the hashtags are the two values I can not see clearly. There are 36 possible values for each (26 letters and 10 numbers), so figuring this out via bruteforce will take a long time. But how many possible numbers could it be?\nWhere\n\\[\n\\text{ Characters to choose from} = n\n\\]\n\\[\nn = 36\n\\]\n\\[\n\\text{ Characters selected} = k\n\\]\n\\[\nk = 2\n\\]"
  },
  {
    "objectID": "posts/combinations/post.html#combinations",
    "href": "posts/combinations/post.html#combinations",
    "title": "Combinations-vs-Permutation",
    "section": "Combinations",
    "text": "Combinations\nPretending that order does not matter for our code or password, we could calculate the combinations by\n\\[\nC_{k}^{\\prime}(n) = \\binom{n + k  - 1}{k}\n\\]\n\\[\nC_{2}^{\\prime}(36) = C_{2}(36 +2 - 1)\n\\]\n\\[\nC_{2}(37) = \\binom{37}{2} = \\frac{37!}{2!(37 - 2)!} = \\frac{37 \\cdot 36}{2 \\cdot 1} = 666\n\\]\nIf order did not matter and we could not have repetition then we would have\n\\[\nC_{k}(n) = \\binom{n}{k} = \\frac{n!}{k!(n - k)!}\n\\]\n\\[\nC_{2}(36) = \\binom{36}{2} = \\frac{36!}{2!(36 - 2)!} = \\frac{36 \\cdot 35}{2 \\cdot 1} = 630\n\\]"
  },
  {
    "objectID": "posts/combinations/post.html#permutation",
    "href": "posts/combinations/post.html#permutation",
    "title": "Combinations-vs-Permutation",
    "section": "Permutation",
    "text": "Permutation\nThis problem will be a permutation problem since the order is important since A and 5 is not the same as 5 and A due to both values being independent from each other. For example,\nA3EA - 3JH5 - 45HL \\(!=\\) A3E5 - 3JHA - 45HL\nThis is a simple solution since repetition is allowed. For example, both values could be the same. As such\n\\[\n\\text{Permutation} = P\n\\]\n\\[\nP_{k}^{\\prime} = n^{k}\n\\]\n\\[\nP_{2}^{\\prime} = 36^{2} = 1296\n\\]\nHowever, lets just say in this situation repetition is not allowed. This would mean if the first character is A then the second character cannot be A. While this may seem like an obscure example in this example, imagine if the code was not entirely scratched off and you were certain that the two values were different from each other.\n\\[\nP_{k}(n) = \\frac{n!}{(n - k)}!\n\\]\n\\[\nP_{2}(36) = \\frac{36!}{(36-2)} = \\frac{36!}{34!} = 36 \\cdot 35 = 1260\n\\]\nThis should be fairly intuitive since 1296 - 1260 = 36."
  },
  {
    "objectID": "posts/combinations/post.html#passwords",
    "href": "posts/combinations/post.html#passwords",
    "title": "Combinations-vs-Permutation",
    "section": "Passwords",
    "text": "Passwords\nTo finish here are the amount of unique combinations for a password of 6 characters\n\n\n\n\n\n\n\n\nCharacters Used\nPossible Characters\nUnique Permutations\n\n\n\n\na - z\n26\n308,915,776\n\n\na - z & 0 - 9\n36\n2,176,782,336\n\n\na - z & A - Z\n52\n19,770,609,664\n\n\na - z & A - Z & 0 - 9\n62\n56,800,235,584\n\n\na - z & A - Z & 0 - 9 & !@#$%^&*()+-\n74\n164,206,490,176\n\n\n\nand 8 characters\n\n\n\n\n\n\n\n\nCharacters Used\nPossible Characters\nUnique Permutations\n\n\n\n\na - z\n26\n208,827,064,576\n\n\na - z & 0 - 9\n36\n2,821,109,907,456\n\n\na - z & A - Z\n52\n53,459,728,531,456\n\n\na - z & A - Z & 0 - 9\n62\n218,340,105,584,896\n\n\na - z & A - Z & 0 - 9 & !@#$%^&*()+-\n74\n899,194,740,203,776\n\n\n\nIf you want to calculate these values in R or Python you can simply do it via\n\nRPython\n\n\nlibrary(tidyr)\n\ncrossing(\n  # total characters \n  character_n = c(26, 36, 52, 62, 74),\n  # password length\n  length = c(6, 7, 8, 10, 12), \n  possible_values = character_n ^ length\n)\n\n\nimport pandas as pd\n\ncharacter_n = [26, 36, 52, 62, 74]\nlength = [6, 7, 8, 10, 12]\n\npd.DataFrame({\n    'character_n': character_n,\n    'length': length,\n    'possible_values': [n ** l for n, l in zip(character_n, length)]\n})\n\n#data = [{'character_n': n, 'length': l, 'possible_values': n ** l} for n, l in zip(character_n, length)]\n#pd.DataFrame.from_records(data)"
  },
  {
    "objectID": "posts/nfl_pca_r/post.html",
    "href": "posts/nfl_pca_r/post.html",
    "title": "NFL PCA w/ R",
    "section": "",
    "text": "Doing a PCA in R is quite easy, but visualising the results can be a challenge. I am going to demonstrate it with NFL data just to add some complexity to it. I wont write much on this post due to time constraints, however, most of the code is commented well enough.\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(gt)\nlibrary(ggrepel)\n\nHow you load the data depends if you want to use the package nflfastR or not. While it has many useful features, you may not want to use it if you already have the data saved that you want to use.\n\n\nShow the code for white_theme\nwhite_theme &lt;- function(base_size = 12, font = \"Lato\") {\n\n  ### Palette\n\n  # http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette\n  cb_palette &lt;- c(\n    \"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n    \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\"\n  )\n\n  # https://www.color-hex.com/color-palette/74767\n  tom_palette &lt;- c(\"#003399\", \"#ff2b4f\", \"#3686d3\", \"#FCAB27\", \"#88398a\")\n\n  # murdoch theme https://gist.github.com/johnburnmurdoch/bd20db77b2582031604ccd1bdc4be582\n  ft_palette &lt;- c(\"#00218D\", \"#FF2B4F\", \"#0083EB\", \"#FCAB27\", \"#FF49EF\")\n\n  # colour pieces\n  scale_fill_cb &lt;- function() {\n    structure(list(ggplot2::scale_fill_manual(values = cb_palette)))\n  }\n  scale_colour_discrete_cb &lt;- function() {\n    structure(list(ggplot2::scale_colour_manual(values = cb_palette)))\n  }\n  scale_color_continuous_cb &lt;- function() {\n    structure(list(ggplot2::scale_color_gradientn(colours = cb_palette)))\n  }\n\n  # Text Setting\n  txt &lt;- ggplot2::element_text(\n    size = base_size + 2, colour = \"black\",\n    face = \"plain\"\n  )\n  bold_txt &lt;- ggplot2::element_text(\n    size = base_size + 2, colour = \"black\",\n    family = \"Montserrat\", face = \"bold\"\n  )\n  large_txt &lt;- ggplot2::element_text(\n    size = base_size + 4, color = \"black\",\n    face = \"bold\"\n  )\n  ggplot2::theme_minimal(base_size = base_size, base_family = font) +\n    ggplot2::theme(\n      # Legend Settings\n      legend.key = element_blank(),\n      legend.background = element_blank(),\n      legend.position = \"bottom\",\n      legend.direction = \"horizontal\",\n      legend.box = \"vertical\",\n      # Backgrounds\n      strip.background = element_rect(),\n      plot.background = element_rect(),\n      plot.margin = unit(c(1, 1, 1, 1), \"lines\"),\n      # Axis & Titles\n      text = txt, axis.text = txt,\n      axis.ticks = element_blank(),\n      axis.line = element_blank(),\n      axis.title = bold_txt,\n      plot.title = large_txt,\n      # Panel\n      panel.grid = element_line(colour = NULL),\n      panel.grid.major = element_line(colour = \"#D2D2D2\"),\n      panel.grid.minor = element_blank()\n    )\n}\n\n\n\nUsing nflfastRWithout nflfastR\n\n\n\nplayers &lt;- nflfastR::load_player_stats(2021)\nrosters &lt;- nflfastR::fast_scraper_roster(2021)\npbp &lt;- nflfastR::load_pbp(2021)\n\nIt is worth noting that you can get multiple seasons at once using this method. E.g. nflfastR::load_player_stats(2020:2021).\n\n\n\n# if you have the data saved locally\ndf &lt;- purrr::map_df(c(2010:2020), function(x) {\n  readRDS(\n    glue::glue(\"~/Documents/nfl/data/pbp/play_by_play_{x}.rds\")\n  )\n})\n\n# if you do not have it saved locally\ndf &lt;- purrr::map_df(c(2010:2020), function(x) {\n  readRDS(\n    glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds\")\n  )\n})\n\nI thought it was worth showing because this was a trick that I found quite useful once I discovered it. I have used it countless times for similar tasks. By using glue(), you can map across the values like an f string in Python.\n\n\n\n\npbp_wr &lt;- pbp |&gt;\n  mutate(\n    # if they caught the ball in the middle of the field, \n    # assign 0 otherwise, assign 1 \n    outside_pass = ifelse(pass_location != \"middle\", 1, 0),\n    pass_air_yards = ifelse(is.na(air_yards), 0, air_yards),\n    pass_air_yards = ifelse(ydstogo &lt;= 10, pass_air_yards, NA)\n  ) |&gt;\n  # rec id is a bit of a hack to keep that value\n  group_by(receiver_id, fantasy_id) |&gt;\n  summarise(\n    rec = sum(complete_pass),\n    air_yards = mean(pass_air_yards, na.rm = TRUE),\n    yards_per_target = mean(yards_gained, na.rm = TRUE),\n    yards_after_catch = mean(yards_after_catch, na.rm = TRUE),\n    td_rate = mean(pass_touchdown),\n    outside_rec = mean(outside_pass, na.rm = TRUE),\n    dist_from_sticks = mean(pass_air_yards - ydstogo, na.rm = TRUE),\n    # first down percentage\n    first_down = mean(first_down, na.rm = TRUE)\n  ) |&gt;\n  # so you don't get random players like a QB\n  filter(rec &gt; 25) |&gt;\n  left_join(\n    pbp |&gt;\n      count(receiver_id, fantasy_id, receiver, posteam) |&gt;\n      group_by(receiver_id) |&gt;\n      arrange(-n) |&gt;\n      # this will keep the first instance of a player\n      # this is to add players non-numerical values\n      mutate(rn = row_number()) |&gt;\n      filter(rn == 1) |&gt;\n      select(-n, -rn)) |&gt;\n  relocate(receiver, .before = rec) |&gt; \n  # this second join is to add the players position\n  # here we are using the gsid, which is why\n  # we wanted the fantasy id before\n  left_join(\n    select(rosters, position, gsis_id), by = c('fantasy_id' = 'gsis_id')\n  ) |&gt;\n  filter(position %in% c('WR', 'TE')) |&gt;\n  # good practice to ungroup at the end\n  ungroup()\n\nIf the dataset is large enough, you may get timeout errors, if that is the case, you can seperate the second left_join() into a seperate call, e.g.\n\n\nShow the code\npbp_wr &lt;- pbp |&gt;\n  mutate(\n    outside_pass = ifelse(pass_location != \"middle\", 1, 0),\n    pass_air_yards = ifelse(is.na(air_yards), 0, air_yards),\n    pass_air_yards = ifelse(ydstogo &lt;= 10, pass_air_yards, NA)\n  ) |&gt;\n  group_by(receiver_id, fantasy_id) |&gt;\n  summarise(\n    rec = sum(complete_pass),\n    air_yards = mean(pass_air_yards, na.rm = TRUE),\n    yards_per_target = mean(yards_gained, na.rm = TRUE),\n    yards_after_catch = mean(yards_after_catch, na.rm = TRUE),\n    td_rate = mean(pass_touchdown),\n    outside_rec = mean(outside_pass, na.rm = TRUE),\n    dist_from_sticks = mean(pass_air_yards - ydstogo, na.rm = TRUE),\n    first_down = mean(first_down, na.rm = TRUE)\n  ) |&gt;\n  filter(rec &gt; 25) |&gt;\n  left_join(\n    pbp |&gt;\n      count(receiver_id, fantasy_id, receiver, posteam) |&gt;\n      group_by(receiver_id) |&gt;\n      arrange(-n) |&gt;\n      mutate(rn = row_number()) |&gt;\n      filter(rn == 1) |&gt;\n      select(-n, -rn)) |&gt;\n  relocate(receiver, .before = rec) \n\npbp_wr &lt;- pbp_wr |&gt; \n  left_join(\n    select(rosters, position, gsis_id), by = c('fantasy_id' = 'gsis_id')\n  ) |&gt;\n  filter(position %in% c('WR', 'TE')) |&gt;\n  ungroup()\n\n\nThis is the resulting dataframe. To run a PCA we only want the numeric columns, however, we will be using the character columns for visualising at a later step.\n\npbp_wr |&gt;\n  gt() |&gt;\n  tab_options(container.height = '300px')\n\n\n\n\n\n\n\n\nreceiver_id\nfantasy_id\nreceiver\nrec\nair_yards\nyards_per_target\nyards_after_catch\ntd_rate\noutside_rec\ndist_from_sticks\nfirst_down\nposteam\nposition\n\n\n\n\n00-0027944\n00-0027944\nJ.Jones\n31\n12.08163265\n6.543860\n4.548387\n0.05263158\n0.8148148\n4.38775510\n0.3157895\nTB\nWR\n\n\n00-0028002\n00-0028002\nR.Cobb\n34\n9.58695652\n7.867925\n5.441176\n0.01886792\n0.6600000\n1.84782609\n0.4150943\nGB\nWR\n\n\n00-0030035\n00-0030035\nA.Thielen\n73\n9.45871560\n6.400000\n2.150685\n0.05000000\n0.7657658\n1.66972477\n0.4750000\nMIN\nWR\n\n\n00-0030061\n00-0030061\nZ.Ertz\n47\n7.03225806\n5.694444\n2.765957\n0.05555556\n0.5942029\n0.00000000\n0.3472222\nARI\nTE\n\n\n00-0030068\n00-0030068\nM.Goodwin\n27\n9.80555556\n8.795455\n4.222222\n0.09090909\n0.7857143\n2.44444444\n0.4090909\nSEA\nWR\n\n\n00-0030279\n00-0030279\nK.Allen\n72\n7.21111111\n7.801887\n4.125000\n0.03773585\n0.7843137\n0.06666667\n0.4339623\nLAC\nWR\n\n\n00-0030431\n00-0030431\nR.Woods\n53\n7.60975610\n5.666667\n2.943396\n0.02150538\n0.7912088\n0.14634146\n0.3440860\nTEN\nWR\n\n\n00-0030564\n00-0030564\nD.Hopkins\n64\n9.25581395\n7.099010\n2.578125\n0.02970297\n0.8750000\n1.87209302\n0.3564356\nARI\nWR\n\n\n00-0031236\n00-0031236\nB.Cooks\n57\n9.21978022\n6.920792\n3.596491\n0.02970297\n0.8924731\n1.36263736\n0.3366337\nHOU\nWR\n\n\n00-0031260\n00-0031260\nL.Thomas\n39\n7.22222222\n5.046875\n3.230769\n0.01562500\n0.6721311\n-0.48148148\n0.2500000\nWAS\nTE\n\n\n00-0031381\n00-0031381\nD.Adams\n100\n11.46875000\n7.937173\n4.930000\n0.07329843\n0.7277778\n3.76250000\n0.3769634\nLV\nWR\n\n\n00-0031408\n00-0031408\nM.Evans\n83\n11.97826087\n8.013245\n2.807229\n0.03973510\n0.7971014\n4.53623188\n0.4370861\nTB\nWR\n\n\n00-0031428\n00-0031428\nAl.Robinson\n33\n9.39583333\n6.396226\n2.151515\n0.05660377\n0.8076923\n2.77083333\n0.4528302\nLA\nWR\n\n\n00-0031544\n00-0031544\nA.Cooper\n78\n10.84000000\n8.169014\n4.166667\n0.06338028\n0.8863636\n3.51200000\n0.4718310\nCLE\nWR\n\n\n00-0031547\n00-0031547\nD.Parker\n31\n15.64285714\n10.780000\n3.387097\n0.06000000\n0.8085106\n7.73809524\n0.5600000\nNE\nWR\n\n\n00-0031549\n00-0031549\nN.Agholor\n31\n12.22727273\n6.581818\n4.741935\n0.03636364\n0.7547170\n4.36363636\n0.2545455\nNE\nWR\n\n\n00-0031588\n00-0031588\nS.Diggs\n119\n10.08383234\n8.438503\n3.689076\n0.05882353\n0.8670520\n2.26946108\n0.4652406\nBUF\nWR\n\n\n00-0031610\n00-0031610\nD.Waller\n28\n11.45000000\n8.083333\n2.857143\n0.06250000\n0.6046512\n3.90000000\n0.4375000\nLV\nTE\n\n\n00-0031763\n00-0031763\nD.Carter\n48\n8.54411765\n7.928571\n4.395833\n0.04285714\n0.8088235\n1.19117647\n0.4571429\nLAC\nWR\n\n\n00-0032211\n00-0032211\nT.Lockett\n90\n9.57272727\n8.060150\n3.111111\n0.06766917\n0.8080000\n1.80000000\n0.4436090\nSEA\nWR\n\n\n00-0032392\n00-0032392\nA.Hooper\n41\n7.78947368\n7.047619\n3.268293\n0.03174603\n0.6666667\n0.26315789\n0.4603175\nTEN\nTE\n\n\n00-0032398\n00-0032398\nC.Moore\n48\n8.73611111\n7.025641\n5.479167\n0.02564103\n0.8243243\n0.75000000\n0.3589744\nHOU\nWR\n\n\n00-0032464\n00-0032464\nK.Raymond\n47\n9.68518519\n9.194030\n4.638298\n0.00000000\n0.7812500\n1.55555556\n0.3880597\nDET\nWR\n\n\n00-0032775\n00-0032775\nD.Robinson\n50\n9.18571429\n6.259259\n3.300000\n0.03703704\n0.7948718\n0.51428571\n0.3333333\nBAL\nWR\n\n\n00-0033009\n00-0033009\nT.Boyd\n64\n8.64367816\n8.793814\n4.109375\n0.05154639\n0.7640449\n1.51724138\n0.5051546\nCIN\nWR\n\n\n00-0033040\n00-0033040\nT.Hill\n126\n11.68047337\n8.905000\n4.103175\n0.03500000\n0.6918919\n3.92307692\n0.4500000\nMIA\nWR\n\n\n00-0033090\n00-0033090\nH.Henry\n41\n7.49056604\n8.344262\n4.951220\n0.03278689\n0.8644068\n0.20754717\n0.3934426\nNE\nTE\n\n\n00-0033282\n00-0033282\nC.Samuel\n64\n6.32954545\n6.560000\n4.515625\n0.04000000\n0.6739130\n-0.72727273\n0.3800000\nWAS\nWR\n\n\n00-0033288\n00-0033288\nG.Kittle\n70\n6.44318182\n8.951923\n6.228571\n0.10576923\n0.6494845\n-1.17045455\n0.4326923\nSF\nTE\n\n\n00-0033307\n00-0033307\nK.Bourne\n35\n10.25581395\n8.509804\n4.142857\n0.01960784\n0.7708333\n2.13953488\n0.4313725\nNE\nWR\n\n\n00-0033466\n00-0033466\nI.McKenzie\n44\n6.85000000\n6.275362\n3.272727\n0.05797101\n0.8059701\n-0.38333333\n0.4202899\nBUF\nWR\n\n\n00-0033536\n00-0033536\nM.Williams\n63\n11.31578947\n9.623656\n5.063492\n0.04301075\n0.8709677\n4.01315789\n0.4193548\nLAC\nWR\n\n\n00-0033572\n00-0033572\nJ.Agnew\n26\n4.41379310\n5.444444\n5.076923\n0.08333333\n0.8235294\n-3.06896552\n0.2777778\nJAX\nWR\n\n\n00-0033591\n00-0033591\nN.Brown\n47\n9.20253165\n6.827586\n3.446809\n0.03448276\n0.7564103\n1.74683544\n0.3908046\nDAL\nWR\n\n\n00-0033757\n00-0033757\nR.Tonyan\n53\n5.06666667\n6.444444\n4.339623\n0.02777778\n0.5522388\n-2.85000000\n0.3333333\nGB\nTE\n\n\n00-0033857\n00-0033857\nJ.Smith-Schuster\n88\n6.54807692\n8.661017\n5.511364\n0.02542373\n0.7610619\n-1.00000000\n0.4915254\nKC\nWR\n\n\n00-0033871\n00-0033871\nC.Davis\n32\n12.30357143\n8.246154\n3.406250\n0.03076923\n0.6562500\n4.87500000\n0.4307692\nNYJ\nWR\n\n\n00-0033881\n00-0033881\nE.Engram\n85\n5.25242718\n7.193548\n6.505882\n0.04032258\n0.7391304\n-2.07766990\n0.3709677\nJAX\nTE\n\n\n00-0033885\n00-0033885\nD.Njoku\n58\n5.25925926\n7.218391\n5.827586\n0.04597701\n0.7375000\n-2.40740741\n0.4252874\nCLE\nTE\n\n\n00-0033891\n00-0033891\nZ.Jones\n95\n8.54687500\n6.848276\n3.178947\n0.04137931\n0.8510638\n1.16406250\n0.3517241\nJAX\nWR\n\n\n00-0033895\n00-0033895\nG.Everett\n64\n5.19753086\n6.937500\n6.265625\n0.05208333\n0.8105263\n-2.59259259\n0.3541667\nLAC\nTE\n\n\n00-0033921\n00-0033921\nC.Godwin\n114\n5.07534247\n6.785276\n5.087719\n0.01840491\n0.6451613\n-2.13698630\n0.3619632\nTB\nWR\n\n\n00-0033943\n00-0033943\nJ.Reynolds\n38\n11.45000000\n7.603175\n3.026316\n0.04761905\n0.7966102\n4.71666667\n0.4444444\nDET\nWR\n\n\n00-0034159\n00-0034159\nW.Dissly\n34\n4.28571429\n8.725000\n5.411765\n0.07500000\n0.7368421\n-4.05714286\n0.5500000\nSEA\nTE\n\n\n00-0034270\n00-0034270\nT.Conklin\n58\n7.20547945\n6.202247\n5.051724\n0.03370787\n0.7471264\n-0.47945205\n0.2808989\nNYJ\nTE\n\n\n00-0034272\n00-0034272\nM.Valdes-Scantling\n49\n12.21052632\n7.931373\n3.836735\n0.03921569\n0.8043478\n4.86315789\n0.4509804\nKC\nWR\n\n\n00-0034286\n00-0034286\nR.James\n68\n6.28767123\n7.233333\n3.235294\n0.04444444\n0.5813953\n-1.38356164\n0.4111111\nNYG\nWR\n\n\n00-0034348\n00-0034348\nC.Sutton\n64\n11.46236559\n7.025424\n2.375000\n0.01694915\n0.8073394\n4.01075269\n0.3728814\nDEN\nWR\n\n\n00-0034351\n00-0034351\nD.Goedert\n71\n5.61445783\n9.163043\n6.661972\n0.04347826\n0.7356322\n-2.77108434\n0.5434783\nPHI\nTE\n\n\n00-0034364\n00-0034364\nJ.Akins\n37\n5.64150943\n8.423729\n7.297297\n0.08474576\n0.7037037\n-1.67924528\n0.4237288\nHOU\nTE\n\n\n00-0034383\n00-0034383\nD.Schultz\n69\n7.16129032\n6.354545\n3.217391\n0.07272727\n0.6822430\n-1.00000000\n0.3272727\nDAL\nTE\n\n\n00-0034411\n00-0034411\nR.Gage\n53\n4.78082192\n5.325301\n3.471698\n0.06024096\n0.7435897\n-2.38356164\n0.3975904\nTB\nWR\n\n\n00-0034487\n00-0034487\nT.Sherfield\n30\n9.65853659\n7.581818\n4.833333\n0.03636364\n0.7735849\n2.17073171\n0.3454545\nMIA\nWR\n\n\n00-0034521\n00-0034521\nA.Lazard\n60\n11.33333333\n7.576923\n4.133333\n0.05769231\n0.7400000\n4.35416667\n0.4615385\nGB\nWR\n\n\n00-0034753\n00-0034753\nM.Andrews\n78\n9.07894737\n7.092308\n3.423077\n0.03846154\n0.6829268\n1.28947368\n0.4307692\nBAL\nTE\n\n\n00-0034764\n00-0034764\nM.Gallup\n44\n10.58227848\n5.402299\n2.636364\n0.05747126\n0.9036145\n3.74683544\n0.3793103\nDAL\nWR\n\n\n00-0034775\n00-0034775\nC.Kirk\n99\n8.40845070\n7.412121\n4.141414\n0.06060606\n0.7329193\n0.66901408\n0.3818182\nJAX\nWR\n\n\n00-0034777\n00-0034777\nD.Chark\n30\n14.70000000\n9.471698\n4.466667\n0.05660377\n0.8076923\n7.58000000\n0.4716981\nDET\nWR\n\n\n00-0034827\n00-0034827\nD.Moore\n63\n11.66666667\n7.023622\n2.888889\n0.05511811\n0.8135593\n3.79824561\n0.3779528\nCAR\nWR\n\n\n00-0034829\n00-0034829\nM.Gesicki\n34\n8.13461538\n6.283333\n2.941176\n0.10000000\n0.6896552\n0.75000000\n0.3666667\nMIA\nTE\n\n\n00-0034830\n00-0034830\nH.Hurst\n65\n5.19480519\n6.166667\n3.876923\n0.03333333\n0.7882353\n-2.33766234\n0.4111111\nCIN\nTE\n\n\n00-0034960\n00-0034960\nJ.Meyers\n67\n9.05434783\n7.532710\n3.552239\n0.05607477\n0.7395833\n1.79347826\n0.4112150\nNE\nWR\n\n\n00-0034970\n00-0034970\nI.Smith\n26\n4.64516129\n4.868421\n3.500000\n0.07894737\n0.8157895\n-2.77419355\n0.2631579\nMIN\nTE\n\n\n00-0034981\n00-0034981\nF.Moreau\n33\n7.86956522\n7.636364\n7.181818\n0.03636364\n0.6296296\n-0.15217391\n0.4363636\nLV\nTE\n\n\n00-0034983\n00-0034983\nH.Renfrow\n36\n6.75555556\n6.346154\n4.138889\n0.03846154\n0.6000000\n-0.77777778\n0.3846154\nLV\nWR\n\n\n00-0035208\n00-0035208\nO.Zaccheaus\n40\n10.44642857\n8.200000\n4.725000\n0.04615385\n0.6721311\n2.62500000\n0.4461538\nATL\nWR\n\n\n00-0035229\n00-0035229\nT.Hockenson\n96\n7.11016949\n7.006757\n4.875000\n0.04054054\n0.7214286\n-0.35593220\n0.3581081\nMIN\nTE\n\n\n00-0035500\n00-0035500\nG.Dortch\n52\n3.77083333\n7.184615\n5.634615\n0.03076923\n0.8281250\n-4.20833333\n0.2923077\nARI\nWR\n\n\n00-0035535\n00-0035535\nD.Slayton\n51\n10.96153846\n8.967033\n6.019608\n0.02197802\n0.8452381\n2.56410256\n0.4395604\nNYG\nWR\n\n\n00-0035639\n00-0035639\nP.Campbell\n63\n5.67073171\n6.422680\n4.380952\n0.03092784\n0.7472527\n-1.64634146\n0.3711340\nIND\nWR\n\n\n00-0035644\n00-0035644\nN.Fant\n51\n5.48333333\n7.100000\n4.549020\n0.05714286\n0.7230769\n-2.25000000\n0.3857143\nSEA\nTE\n\n\n00-0035659\n00-0035659\nT.McLaurin\n77\n11.89090909\n9.377953\n5.116883\n0.03937008\n0.7500000\n3.83636364\n0.4881890\nWAS\nWR\n\n\n00-0035662\n00-0035662\nM.Brown\n67\n10.48484848\n6.059829\n3.522388\n0.02564103\n0.8598131\n2.94949495\n0.3333333\nARI\nWR\n\n\n00-0035676\n00-0035676\nA.Brown\n101\n11.85333333\n9.173184\n5.930693\n0.06703911\n0.8143713\n4.02666667\n0.4078212\nPHI\nWR\n\n\n00-0035689\n00-0035689\nD.Knox\n56\n7.75675676\n7.525000\n3.857143\n0.08750000\n0.8441558\n-0.36486486\n0.4250000\nBUF\nTE\n\n\n00-0035719\n00-0035719\nD.Samuel\n69\n4.73267327\n7.025000\n9.391304\n0.02500000\n0.7304348\n-3.08910891\n0.2916667\nSF\nWR\n\n\n00-0036040\n00-0036040\nJ.Johnson\n42\n8.29824561\n7.582090\n4.214286\n0.10447761\n0.6769231\n1.05263158\n0.4179104\nNO\nTE\n\n\n00-0036165\n00-0036165\nI.Hodgins\n46\n8.45614035\n7.843750\n2.673913\n0.07812500\n0.7118644\n1.07017544\n0.4531250\nNYG\nWR\n\n\n00-0036232\n00-0036232\nH.Bryant\n31\n5.32432432\n5.431818\n3.645161\n0.02272727\n0.7380952\n-2.18918919\n0.2727273\nCLE\nTE\n\n\n00-0036233\n00-0036233\nD.Peoples-Jones\n61\n11.15555556\n8.067308\n3.131148\n0.02884615\n0.8333333\n3.44444444\n0.3750000\nCLE\nWR\n\n\n00-0036244\n00-0036244\nC.Parkinson\n28\n7.34210526\n8.195122\n5.535714\n0.04878049\n0.8250000\n0.15789474\n0.3658537\nSEA\nTE\n\n\n00-0036252\n00-0036252\nM.Pittman\n99\n6.62686567\n6.292517\n3.606061\n0.02721088\n0.6241135\n-0.80597015\n0.3673469\nIND\nWR\n\n\n00-0036259\n00-0036259\nJ.Jennings\n39\n8.23636364\n7.546875\n4.897436\n0.01562500\n0.7301587\n0.96363636\n0.4375000\nSF\nWR\n\n\n00-0036261\n00-0036261\nB.Aiyuk\n84\n9.79130435\n8.388060\n4.904762\n0.05970149\n0.7016129\n1.40000000\n0.4402985\nSF\nWR\n\n\n00-0036268\n00-0036268\nL.Shenault\n27\n-0.04166667\n8.500000\n12.222222\n0.03125000\n0.9062500\n-7.37500000\n0.3125000\nCAR\nWR\n\n\n00-0036290\n00-0036290\nC.Kmet\n50\n6.39393939\n7.452055\n5.720000\n0.09589041\n0.7826087\n-1.59090909\n0.3972603\nCHI\nTE\n\n\n00-0036309\n00-0036309\nD.Mooney\n40\n12.16071429\n7.825397\n4.100000\n0.03174603\n0.8688525\n4.33928571\n0.3333333\nCHI\nWR\n\n\n00-0036322\n00-0036322\nJ.Jefferson\n135\n9.49152542\n9.024155\n4.626866\n0.03864734\n0.8437500\n2.12429379\n0.4589372\nMIN\nWR\n\n\n00-0036326\n00-0036326\nC.Claypool\n46\n9.44927536\n5.573171\n3.239130\n0.01219512\n0.8354430\n1.63768116\n0.3170732\nPIT\nWR\n\n\n00-0036345\n00-0036345\nK.Osborn\n62\n7.44827586\n6.767677\n4.903226\n0.06060606\n0.7311828\n-0.32183908\n0.3737374\nMIN\nWR\n\n\n00-0036358\n00-0036358\nC.Lamb\n121\n9.54037267\n8.437158\n4.495868\n0.05464481\n0.7142857\n1.90062112\n0.4371585\nDAL\nWR\n\n\n00-0036407\n00-0036407\nJ.Jeudy\n67\n11.19148936\n9.169811\n5.850746\n0.05660377\n0.8300000\n3.28723404\n0.4245283\nDEN\nWR\n\n\n00-0036410\n00-0036410\nT.Higgins\n87\n10.18548387\n8.435714\n3.678161\n0.05714286\n0.8461538\n2.65322581\n0.4214286\nCIN\nWR\n\n\n00-0036613\n00-0036613\nJ.Waddle\n78\n11.82727273\n10.687023\n6.615385\n0.06106870\n0.6532258\n4.10909091\n0.5038168\nMIA\nWR\n\n\n00-0036637\n00-0036637\nN.Gray\n31\n4.97142857\n8.615385\n6.516129\n0.02564103\n0.7894737\n-3.14285714\n0.3846154\nKC\nTE\n\n\n00-0036862\n00-0036862\nB.Skowronek\n39\n6.44230769\n6.064516\n3.512821\n0.00000000\n0.7868852\n-1.34615385\n0.2741935\nLA\nWR\n\n\n00-0036876\n00-0036876\nK.Granson\n31\n5.94736842\n6.863636\n4.096774\n0.00000000\n0.8250000\n-2.26315789\n0.3863636\nIND\nTE\n\n\n00-0036894\n00-0036894\nP.Freiermuth\n63\n7.65555556\n7.106796\n4.619048\n0.01941748\n0.6530612\n0.03333333\n0.3786408\nPIT\nTE\n\n\n00-0036900\n00-0036900\nJ.Chase\n107\n8.41290323\n7.447059\n5.000000\n0.06470588\n0.8148148\n0.90967742\n0.4352941\nCIN\nWR\n\n\n00-0036936\n00-0036936\nR.Moore\n41\n5.66666667\n6.786885\n6.853659\n0.01639344\n0.8571429\n-2.29411765\n0.3114754\nARI\nWR\n\n\n00-0036955\n00-0036955\nT.Marshall\n28\n11.16666667\n10.489796\n4.714286\n0.02040816\n0.9361702\n3.33333333\n0.5102041\nCAR\nWR\n\n\n00-0036963\n00-0036963\nA.St. Brown\n106\n6.77862595\n8.047945\n4.962264\n0.04109589\n0.7739726\n-0.84732824\n0.4657534\nDET\nWR\n\n\n00-0036980\n00-0036980\nE.Moore\n37\n11.33928571\n6.521739\n2.621622\n0.01449275\n0.8461538\n3.50000000\n0.3478261\nNYJ\nWR\n\n\n00-0036988\n00-0036988\nJ.Palmer\n74\n7.60000000\n6.520325\n3.567568\n0.02439024\n0.8761062\n0.30476190\n0.3902439\nLAC\nWR\n\n\n00-0037247\n00-0037247\nG.Pickens\n52\n12.63750000\n8.364583\n2.000000\n0.04166667\n0.8452381\n5.21250000\n0.4479167\nPIT\nWR\n\n\n00-0037545\n00-0037545\nR.Shaheed\n28\n11.45161290\n14.352941\n6.071429\n0.05882353\n0.8529412\n3.70967742\n0.5294118\nNO\nWR\n\n\n00-0037664\n00-0037664\nA.Pierce\n41\n10.34848485\n7.144578\n2.756098\n0.02409639\n0.8333333\n2.19696970\n0.3734940\nIND\nWR\n\n\n00-0037740\n00-0037740\nG.Wilson\n82\n9.86923077\n7.342105\n4.792683\n0.02631579\n0.7808219\n2.20769231\n0.3947368\nNYJ\nWR\n\n\n00-0037742\n00-0037742\nT.Burks\n33\n10.82352941\n7.655172\n4.878788\n0.01724138\n0.7222222\n3.17647059\n0.3793103\nTEN\nWR\n\n\n00-0037744\n00-0037744\nT.McBride\n29\n5.65625000\n6.309524\n4.034483\n0.02380952\n0.8717949\n-2.15625000\n0.3095238\nARI\nTE\n\n\n00-0037809\n00-0037809\nC.Okonkwo\n32\n8.71794872\n9.416667\n7.937500\n0.06250000\n0.8043478\n0.87179487\n0.4583333\nTEN\nTE\n\n\n00-0037816\n00-0037816\nR.Doubs\n42\n9.51724138\n6.159420\n4.666667\n0.04347826\n0.7910448\n1.84482759\n0.3043478\nGB\nWR\n\n\n00-0037838\n00-0037838\nI.Likely\n36\n6.64705882\n5.828125\n4.194444\n0.04687500\n0.5833333\n-1.29411765\n0.3281250\nBAL\nTE\n\n\n00-0038090\n00-0038090\nS.Moore\n27\n6.67441860\n6.068182\n6.259259\n0.02272727\n0.6904762\n-1.74418605\n0.3409091\nKC\nWR\n\n\n00-0038115\n00-0038115\nD.Bellinger\n33\n3.97368421\n6.928571\n4.818182\n0.07142857\n0.7250000\n-3.68421053\n0.3809524\nNYG\nTE\n\n\n00-0038124\n00-0038124\nC.Watson\n41\n13.90625000\n8.985294\n6.731707\n0.10294118\n0.7272727\n6.65625000\n0.4411765\nGB\nWR\n\n\n00-0038129\n00-0038129\nC.Otton\n46\n4.50684932\n5.756410\n4.695652\n0.02564103\n0.6805556\n-2.94520548\n0.3076923\nTB\nTE\n\n\n\n\n\n\n\n\npca_fit &lt;- pbp_wr |&gt;\n  # only keep numerical columns\n  select(where(is.numeric)) |&gt; \n  # scale data\n  scale() |&gt; \n  # PCA\n  prcomp() \n\npca_fit\n\nStandard deviations (1, .., p=8):\n[1] 1.68173088 1.27174932 1.07711637 0.98588505 0.87729801 0.71479075 0.36584202\n[8] 0.08870096\n\nRotation (n x k) = (8 x 8):\n                         PC1        PC2          PC3          PC4         PC5\nrec                0.1159268 -0.1197363  0.383822308 -0.853131887  0.20655670\nair_yards          0.5310677  0.2788281 -0.032409869  0.074971563 -0.12053703\nyards_per_target   0.4261138 -0.4524480 -0.229000421 -0.017577601 -0.17037049\nyards_after_catch -0.1357212 -0.6116066 -0.379536188 -0.002394961 -0.12774032\ntd_rate            0.1416324 -0.3082276  0.436150400  0.471041296  0.65943710\noutside_rec        0.1507499  0.1354033 -0.665765246 -0.189232967  0.66372069\ndist_from_sticks   0.5301450  0.2892646 -0.009558681  0.083941277 -0.09075131\nfirst_down         0.4250833 -0.3635023  0.146898679 -0.038670181 -0.11765724\n                          PC6         PC7          PC8\nrec               -0.23222341 -0.02181242  0.001206479\nair_yards         -0.32207027  0.11590137  0.708420325\nyards_per_target   0.04860741 -0.72673063 -0.037966166\nyards_after_catch -0.52360740  0.41590290 -0.002470946\ntd_rate           -0.18938486 -0.04210135  0.018234833\noutside_rec        0.18362831  0.07475244  0.007449410\ndist_from_sticks  -0.30894140  0.16913733 -0.704163821\nfirst_down         0.63307572  0.49900960  0.021376681\n\n\n\n\nInstead of calling scale(), there is an option to have prcomp() scale the data via prcomp(scale = TRUE).\n\n# helper for the axis labels\nimportance &lt;- pca_fit |&gt; \n  tidy(matrix = \"eigenvalues\") |&gt; \n  filter(PC %in% c(1, 2)) |&gt; \n  pull(percent) |&gt; \n  round(3)\n\npca_fit |&gt;\n  # add original dataset back in\n  augment(pbp_wr) |&gt; \n  ggplot(aes(\n    .fittedPC1, \n    .fittedPC2, \n    colour = position)) + \n  geom_point(size = 1.5) +\n  labs(\n    x = paste0('PC1 (Accounts for ', importance[[1]]*100, '% of Variance)'), \n    y = paste0('PC2 (Accounts for ', importance[[2]]*100, '% of Variance)') \n    ) +\n  white_theme()\n\n\n\n\nTo look how the different features contribute to each component, we can do:\n\npca_fit |&gt;\n  tidy(matrix = \"rotation\") |&gt;\n  pivot_wider(\n    names_from = \"PC\", \n    names_prefix = \"PC\", \n    values_from = \"value\") |&gt;\n  gt() |&gt;\n  tab_options(container.height = '500px')\n\n\n\n\n\n\n\n\ncolumn\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\n\n\n\n\nrec\n0.1159268\n-0.1197363\n0.383822308\n-0.853131887\n0.20655670\n-0.23222341\n-0.02181242\n0.001206479\n\n\nair_yards\n0.5310677\n0.2788281\n-0.032409869\n0.074971563\n-0.12053703\n-0.32207027\n0.11590137\n0.708420325\n\n\nyards_per_target\n0.4261138\n-0.4524480\n-0.229000421\n-0.017577601\n-0.17037049\n0.04860741\n-0.72673063\n-0.037966166\n\n\nyards_after_catch\n-0.1357212\n-0.6116066\n-0.379536188\n-0.002394961\n-0.12774032\n-0.52360740\n0.41590290\n-0.002470946\n\n\ntd_rate\n0.1416324\n-0.3082276\n0.436150400\n0.471041296\n0.65943710\n-0.18938486\n-0.04210135\n0.018234833\n\n\noutside_rec\n0.1507499\n0.1354033\n-0.665765246\n-0.189232967\n0.66372069\n0.18362831\n0.07475244\n0.007449410\n\n\ndist_from_sticks\n0.5301450\n0.2892646\n-0.009558681\n0.083941277\n-0.09075131\n-0.30894140\n0.16913733\n-0.704163821\n\n\nfirst_down\n0.4250833\n-0.3635023\n0.146898679\n-0.038670181\n-0.11765724\n0.63307572\n0.49900960\n0.021376681\n\n\n\n\n\n\n\n\n# define arrow style for plotting\narrow_style &lt;- arrow(\n  angle = 20, \n  ends = \"first\", \n  type = \"closed\", \n  length = grid::unit(8, \"pt\")\n  )\n\n\n# plot rotation matrix\npca_fit |&gt;\n  tidy(matrix = \"rotation\") |&gt;\n  pivot_wider(\n    names_from = \"PC\", \n    names_prefix = \"PC\", \n    values_from = \"value\") |&gt;\n  ggplot(aes(PC1, PC2)) +\n  scale_color_brewer(palette = \"Accent\", direction = 1) + \n  geom_segment(\n    aes(colour = column),\n    xend = 0, \n    yend = 0, \n    arrow = arrow_style) +\n  geom_text_repel( \n    aes(label = column, colour = column),\n    hjust = 1, \n    nudge_x = -0.02\n  ) +\n  scale_x_continuous(limits = c(-1, 1)) + \n  scale_y_continuous(limits = c(-1, 1)) +\n  white_theme() +\n  theme(legend.position = \"none\")\n\n\n\n\nI used a dark theme for this plot because of the different colours used for the components.\n\npca_fit |&gt;\n  tidy(matrix = \"eigenvalues\") |&gt;\n  gt() |&gt;\n  tab_options(container.height = '500px')\n\n\n\n\n\n\n\n\nPC\nstd.dev\npercent\ncumulative\n\n\n\n\n1\n1.68173088\n0.35353\n0.35353\n\n\n2\n1.27174932\n0.20217\n0.55570\n\n\n3\n1.07711637\n0.14502\n0.70072\n\n\n4\n0.98588505\n0.12150\n0.82221\n\n\n5\n0.87729801\n0.09621\n0.91842\n\n\n6\n0.71479075\n0.06387\n0.98229\n\n\n7\n0.36584202\n0.01673\n0.99902\n\n\n8\n0.08870096\n0.00098\n1.00000\n\n\n\n\n\n\n\nLikewise, to see how many components to use, we can look at the cumulative explained variance.\n\npca_fit |&gt;\n  tidy(matrix = \"eigenvalues\") |&gt;\n  ggplot(aes(PC, percent)) +\n  geom_col(fill = \"#56B4E9\", alpha = 0.8) +\n  scale_x_continuous(breaks = 1:8) +\n  scale_y_continuous(\n    breaks = seq(0, 0.40, 0.05),\n    labels = scales::percent_format(), \n    expand = expansion(mult = c(0, 0.01))\n  ) +\n  white_theme()\n\n\n\n\n\nUpdate\nI thought it might be fun to add an example of a 3D plot of the components. For this I will use plotly. It is one of the more interactive friendly visualisation tools, and from my experience the code is very similar from Python to R. If you are not too familiar with Python, writing this code may feel awkward. However, I think it is quite useful to get familiar with it because I can easily use plotly in Python without having to change much at all.\n\nlibrary(plotly)\n\n# making a seperate df will simplify plotly creation\npca_df &lt;- pca_fit |&gt;\n  augment(pbp_wr)\n\nfig &lt;- plot_ly(\n  pca_df, \n  x = ~.fittedPC1, \n  y = ~.fittedPC2, \n  z = ~.fittedPC3, \n  color = ~position, \n  colors = c('#386cb0', '#beaed4'), \n  text = ~receiver)\n\nfig &lt;- fig |&gt; \n  add_markers()\n\nfig &lt;- fig |&gt; \n  layout(scene = list(\n    xaxis = list(title = 'PC1'),\n    yaxis = list(title = 'PC2'),\n    zaxis = list(title = 'PC3')\n    ))\n\n\nfig\n\n\n\n\n\nThat is all for now. I have a few more similar posts saved in my draft, so they should be on here soon (within the next six months).\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] plotly_4.10.1   ggrepel_0.9.3   gt_0.8.0        broom_1.0.4    \n [5] lubridate_1.9.2 forcats_0.5.1   stringr_1.5.0   dplyr_1.1.1    \n [9] purrr_1.0.1     readr_2.1.4     tidyr_1.3.0     tibble_3.2.1   \n[13] ggplot2_3.4.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0   xfun_0.37          colorspace_2.1-0   vctrs_0.6.1       \n [5] generics_0.1.3     viridisLite_0.4.1  htmltools_0.5.5    yaml_2.3.7        \n [9] utf8_1.2.3         rlang_1.1.0        pillar_1.9.0       glue_1.6.2        \n[13] withr_2.5.0        bit64_4.0.5        RColorBrewer_1.1-3 lifecycle_1.0.3   \n[17] munsell_0.5.0      gtable_0.3.3       htmlwidgets_1.6.2  evaluate_0.20     \n[21] labeling_0.4.2     knitr_1.42         tzdb_0.3.0         fastmap_1.1.1     \n[25] crosstalk_1.2.0    parallel_4.1.2     fansi_1.0.4        Rcpp_1.0.10       \n[29] scales_1.2.1       backports_1.4.1    vroom_1.6.1        jsonlite_1.8.4    \n[33] farver_2.1.1       bit_4.0.5          hms_1.1.3          digest_0.6.31     \n[37] stringi_1.7.12     grid_4.1.2         cli_3.6.1          tools_4.1.2       \n[41] magrittr_2.0.3     sass_0.4.5         lazyeval_0.2.2     crayon_1.5.2      \n[45] pkgconfig_2.0.3    ellipsis_0.3.2     data.table_1.14.8  timechange_0.2.0  \n[49] httr_1.4.5         rmarkdown_2.20     rstudioapi_0.14    R6_2.5.1          \n[53] compiler_4.1.2"
  },
  {
    "objectID": "posts/nfl_pca_python/post.html",
    "href": "posts/nfl_pca_python/post.html",
    "title": "NFL PCA w/ Python",
    "section": "",
    "text": "This is like the previous post on how to do a PCA for NFL WR, however, this time the focus will be on using Python. For the most part it is the same, however, there will be differences with the set up.\nI did not expect it to be this complicated, but after a bit of work, I was able to get it done in a fairly similar way to what I did in R. One of the first differences is that pandas does not allow multiple assign statements for the same column in a single .assign() call, so we have to use a nested ifelse1 statement. Next, I had to define a custom function for dist_from_sticks, which slowed down computation by a lot. This is something that is worth coming back to because there must be an easier approach.\nWithout that, the block takes roughly 10 seconds to run, whereas with it, that goes around a minute. Because of that, I decided to break down to join functions below.\nThen running the pca. Slightly more steps than in R, but sklearn makes it easy enough.\nThe explained variance plot looks the same."
  },
  {
    "objectID": "posts/nfl_pca_python/post.html#footnotes",
    "href": "posts/nfl_pca_python/post.html#footnotes",
    "title": "NFL PCA w/ Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nnp.where() is the same as ifelse in base R.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "becausejustyn",
    "section": "",
    "text": "NFL Elo Code\n\n\n\n\n\n\n\nPython\n\n\nNFL\n\n\nR\n\n\n\n\nImplementing Elo with code.\n\n\n\n\n\n\nAug 30, 2023\n\n\nJustyn Rodrigues\n\n\n\n\n\n\n  \n\n\n\n\nNFL Elo\n\n\n\n\n\n\n\nR\n\n\nNFL\n\n\n\n\nIntroduction to Elo.\n\n\n\n\n\n\nAug 30, 2023\n\n\nJustyn Rodrigues\n\n\n\n\n\n\n  \n\n\n\n\nNFL PCA w/ Python\n\n\n\n\n\n\n\nPython\n\n\nNFL\n\n\n\n\nComparing the different NFL WR.\n\n\n\n\n\n\nSep 14, 2022\n\n\nJustyn Rodrigues\n\n\n\n\n\n\n  \n\n\n\n\nCode Snippets\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nCommand Line\n\n\n\n\nCode snippets.\n\n\n\n\n\n\nSep 7, 2022\n\n\nJustyn Rodrigues\n\n\n\n\n\n\n  \n\n\n\n\nNFL PCA w/ R\n\n\n\n\n\n\n\nR\n\n\nNFL\n\n\n\n\nComparing the different NFL WR.\n\n\n\n\n\n\nJul 4, 2022\n\n\nJustyn Rodrigues\n\n\n\n\n\n\n  \n\n\n\n\nColouring Text in ggplot2\n\n\n\n\n\n\n\nR\n\n\nggplot\n\n\n\n\nHow to colour text in ggplot.\n\n\n\n\n\n\nMar 26, 2022\n\n\nJustyn Rodrigues\n\n\n\n\n\n\n  \n\n\n\n\nCombinations-vs-Permutation\n\n\n\n\n\n\n\nProbability\n\n\n\n\nComparing combinations and permutations.\n\n\n\n\n\n\nDec 26, 2021\n\n\nJustyn Rodrigues\n\n\n\n\n\n\n  \n\n\n\n\nBarplots\n\n\n\n\n\n\n\nR\n\n\nData Viz\n\n\n\n\nSome different ways you can adjust barplots.\n\n\n\n\n\n\nFeb 11, 2021\n\n\nJustyn Rodrigues\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nfl_elo1/post.html",
    "href": "posts/nfl_elo1/post.html",
    "title": "NFL Elo",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nOne of my favourite models that I have come across is the Elo ratings. In short, it is a system that can be applied to pairwise matchups such as teams or players. It is quite popular across different domains such as chess, education, chess, online gaming, and many more. I like it because it was the first time I tried adjusting a formula with some knowledge about the topic that I wanted to focus on. Elo updates after every single game a player or team participates in. Because a team can only play the teams on their schedule in sport, Elo allows us to reduce some of the uncertainty to how good a particular team is. Another nice feature of Elo is that it rewards teams more for beating a good team, and punishes good teams for losing to bad teams1. Thus, each team has some unobserved ability parameter, which we try to estimate given our observed data. Elo at its most basic level will calculate the probability that \\(Team_{i}\\) beats \\(Team_{j}\\) by taking the difference of their ability parameter, then squeezing it through a sigmoid shape to return a value between \\(0\\) and \\(1\\)."
  },
  {
    "objectID": "posts/nfl_elo1/post.html#overview-of-elo",
    "href": "posts/nfl_elo1/post.html#overview-of-elo",
    "title": "NFL Elo",
    "section": "Overview of Elo",
    "text": "Overview of Elo\nA matchup of \\(Team_{i}\\) and \\(Team_{j}\\) start a match with rankings \\(R_{i}\\) and \\(R_{j}\\). The score of the game is then coded as \\(0\\) for a loss, \\(0.5\\) for a draw and \\(1\\) for a win. The priors can be expressed using:\n\\[P_{i} = \\frac{1}{1 + 10^{(R_{j} - R_{i})/400}} \\quad P_{j} = 1 - P_{i}\\]\n\n\nWhere \\(R\\) is short for ranking.\nwhere \\(P_{i}\\) is the prior probability that team \\(i\\) wins the match. After each match, ratings are updated as follows:\n\\[R_{i}^{new} = R_{i} + K(S_{i} - P_{i}) \\quad R_{j}^{new} = R_{j} + K(S_{j} - P_{j})\\]\nwhere \\(S_{i}\\) is the score of team \\(i\\) (0/0.5/1) and \\(K\\) is an update weight (commonly called the k-factor).\n\n\n\n\n\n\nNote\n\n\n\nSometimes K Factor is simply called K or Update Factor.\n\nA larger \\(K\\) creates more variance as the values get updated, whereas a lower \\(K\\) value takes longer to adjust to new information. 538 in their Elo model use a \\(K\\) of \\(25\\). 2\nIn otherwords, it is a value to update a teams Elo rating.\n\n\n\n\\[\\underbrace{Elo_{Team_{i}}}_{\\text{new Elo}} = \\underbrace{Elo_{Team_{i}}}_{\\text{old Elo}} + \\text{ K Factor } \\Big (\\text{ Actual Score }_{Team_{i}} - \\text{ Expected Score }_{Team_{i}}\\Big )\\]\nWhere\n\nActual score is the outcome mapped to 0/0.5/1 depending on the outcome.\nExpected Score: is the win probability for \\(Team_{i}\\)\n\nAs such, it is a zero sum game. For every elo point one team gains, the other team loses the same amount.\n\nScaling Factor\nA scaling factor of \\(400\\) means that a difference in \\(400\\) Elo would give the favoured team a \\(90\\%\\) chance to win (see below). A smaller value would decrease the range of values. This does not matter too much, however, I like to keep this at \\(400\\) so comparing different Elo methods is apples to apples. We can simply look at a scaling factor of \\(400\\) as:\n\n\n\nElo Diff\nWin Probability\n\n\n\n\n+100\n64%\n\n\n+200\n75%\n\n\n+400\n91%\n\n\n\nIn code to see this, we can simple do:\ndf = pd.DataFrame({\n    'elo_diff': np.arange(-1000, 1100, 100),\n    # [::-1] reverses the order of the prob column so -1000 is the underdog\n    'prob': 1 / (1 + 10**((np.arange(-1000, 1100, 100)) / 400))[::-1]\n})\n\nfig = plt.figure(figsize = (8, 4))\n\nplt.plot(df['elo_diff'], df['prob'], color = '#2ca25f', linewidth = 2)\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.xlabel('Elo Difference')\nplt.ylabel('Win Probability')\nplt.title('Probability that team $i$ beats team $j$')\nplt.grid(True)\nplt.show()\n\n\n\nElo Win Probability"
  },
  {
    "objectID": "posts/nfl_elo1/post.html#example",
    "href": "posts/nfl_elo1/post.html#example",
    "title": "NFL Elo",
    "section": "Example",
    "text": "Example\nFor example, say \\(Team_{i}\\) has an Elo rating of \\(1600\\) and \\(Team_{j}\\) has a rating of \\(1500\\), then we simple input the values into the formula, giving \\(Team_{i}\\) a \\(64\\%\\) chance of winning.\n\\[\\text{Expected Score/Win Probability} = \\frac{1}{1+10^{\\frac{(1500 - 1600)}{400}}} = \\frac{1}{1 + 10^{\\frac{-100}{400}}} = 0.64\\]"
  },
  {
    "objectID": "posts/nfl_elo1/post.html#adjustments",
    "href": "posts/nfl_elo1/post.html#adjustments",
    "title": "NFL Elo",
    "section": "Adjustments",
    "text": "Adjustments\n\nHome Field Advantage\nThis is probably the most used adjustment. In short, we provide the home team with \\(x\\) additional Elo points. But how many should we provide? 538 traditionally use \\(65\\), which translates to roughly a \\(10\\%\\) increase in win probability. I thought this was a bit high so I ran some quick math to see how often the home team wins.\nimport numpy as np\nimport pandas as pd\n\n# I have pbp stored in parquet files\nimport pyarrow.dataset as ds\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')\nnp.set_printoptions(suppress = True)\npd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:.2f}'.format\n\nDATA_PATH = '../../Documents/nfl/data/pbp'\nDATA_COLS = ['game_id', 'season', 'home_team', 'away_team', 'season_type', 'week', 'game_date', 'home_score', 'away_score', 'result']\n\nreplace_dict = {\n    'SD': 'LAC',\n    'OAK': 'LV',\n    'STL': 'LA'\n}\n\ndataset = ds.dataset(DATA_PATH, format = 'parquet')\ndf = (\n    dataset\n    .to_table(columns = DATA_COLS)\n    .to_pandas()\n    .drop_duplicates(subset = ['game_id'])\n    .reset_index(drop = True)\n    .replace({'home_team': replace_dict, 'away_team': replace_dict})\n    .assign(home_win = lambda x: np.where(x['home_score'] &gt; x['away_score'], 1, 0))\n    )\n    \ndf['home_win'].mean()\n0.5648177002181365\nThat is a bit of a difference. Just to be sure, I plotted it by season to get a visual incase there was something funky going on.\nplt.figure(figsize = (9, 4))\n\nyears = range(1999, 2022, 1)\nyear_lst = [datetime(year, 1, 1).strftime(\"'%y\") for year in years]\n\nplt.axhline(df['home_win'].mean(), color = 'black', linestyle = '--')\nplt.plot(df.groupby('season')['home_win'].mean(), marker = 'o', color = '#2ca25f')\nplt.xticks(years, year_lst)\n\nplt.title('Home Win Percentage by Season')\nplt.xlabel('Season')\nplt.ylabel('Home Win Percentage')\nplt.grid(True)\nplt.show()\n\n\n\nHome Win Probability\n\n\nThe plot checks out with my initial intuition. A \\(5\\%\\) added probability seems more appripriate.\n\n\nYearly\n538 regress a teams ELO from the previous year by \\(\\frac{2}{3}\\) towards the baseline of \\(1500\\). This can be achieved via:\n\\[ELO_{\\text{current year}} = ELO_{\\text{previous year}} \\times \\frac{2}{3} + 1500 \\times \\frac{1}{3}\\]\n\n\nMargin of Victory (MOV)\n\\[\\ln(\\left| MOV \\right| + 1)\\]\nBy using the log function, a shrinkage effect occurs where blowout results are discounted. For example, winning a game by 28 points is not much more useful than winning by 21 points. This also addresses situations where a team can inflate their ranking by having a very large win.\n\\[\\text{Margin of Victory Multiplier} = ln(|\\text{PointDiff}| +1) \\times \\frac{2.2}{|Elo_{i} - Elo_{j}| \\times 0.001 + 2.2}\\]"
  },
  {
    "objectID": "posts/nfl_elo1/post.html#next",
    "href": "posts/nfl_elo1/post.html#next",
    "title": "NFL Elo",
    "section": "Next",
    "text": "Next\nThis post has gotten long enough, so I will save the code for the next post."
  },
  {
    "objectID": "posts/nfl_elo1/post.html#footnotes",
    "href": "posts/nfl_elo1/post.html#footnotes",
    "title": "NFL Elo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTeams or players through a season probably do not get better or worse, but our estimation of their skill improves, or the uncertainty decreases.↩︎\nIn competitions that have a longer season, such as chess, it has been suggested that variable K-Factors are used, where it increases based on the elo values to account for lower rating players more likely to have higher variance, whereas those near the top tend to be more consistent.↩︎"
  },
  {
    "objectID": "posts/nfl_elo2/post.html",
    "href": "posts/nfl_elo2/post.html",
    "title": "NFL Elo Code",
    "section": "",
    "text": "Continuing from the previous post on calculating Elo, here is a quick implementation with code.\nImports are pretty standard.\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport pyarrow.dataset as ds\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\nplt.style.use('ggplot')\n# scientific notation off\nnp.set_printoptions(suppress = True)\npd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:.2f}'.format\n\nDATA_PATH = '../../Documents/nfl/data/pbp'\n# do not need all 366 columns\nDATA_COLS = ['game_id', 'season', 'home_team', 'away_team', 'season_type', 'week', 'game_date', 'home_score', 'away_score', 'result']\n\n# correct name changes\nreplace_dict = {\n    'SD': 'LAC',\n    'OAK': 'LV',\n    'STL': 'LA'\n}\n\ndataset = ds.dataset(DATA_PATH, format = 'parquet')\ndf = (\n    dataset\n    .to_table(columns = DATA_COLS)\n    .to_pandas()\n    .drop_duplicates(subset = ['game_id'])\n    .reset_index(drop = True)\n    .replace({'home_team': replace_dict, 'away_team': replace_dict})\n    .assign(home_win = lambda x: np.where(x['home_score'] &gt; x['away_score'], 1, 0))\n    )\n\n# elo functions\ndef calculate_expected_score(home_team_rating, away_team_rating, hfa=False):\n    if hfa:\n        home_team_rating += 40\n    return 1 / (1 + 10 ** ((home_team_rating - away_team_rating) / 400))\n\ndef calculate_new_rating(team_rating, observed_score, expected_score, k=20):\n    return team_rating + k * (observed_score - expected_score)\nFirst we want to loop through each season to calculate the Elo ratings, then for the next season, regress the elo rating per the previous post.\nelo_ratings_dict = {}\n\n# Initialize ELO ratings for the first season\ninitial_elo_ratings = pd.DataFrame({\n    'team': sorted(df['home_team'].unique().tolist()),\n    'elo_rating': 1500,  # Starting ELO rating for all teams\n    'season': df['season'].min(),  # Set to the first season in your dataset\n    'week': 0\n})\n\nfor season in df['season'].unique():\n    df_season = df.query('season == @season').sort_values('game_date', ascending=True).reset_index(drop=True)\n    \n    elo_ratings = initial_elo_ratings.copy()  # Initialize ELO ratings for the current season\n    \n    if season &gt; df['season'].min():\n        prev_season = season - 1\n        elo_ratings['elo_rating'] = elo_ratings_dict[(prev_season, df_season['week'].min())]['elo_rating'] * 2/3 + 1500 * 1/3\n    \n    for week in df_season['week'].unique():\n        df_week = df_season[df_season['week'] == week]\n\n        for game_i in range(df_week.shape[0]):\n            # Get data for the current game\n            home_team = df_week.iloc[game_i][\"home_team\"]\n            away_team = df_week.iloc[game_i][\"away_team\"]\n            home_score = df_week.iloc[game_i][\"home_win\"]\n            game_week = df_week.iloc[game_i][\"week\"]\n            game_season = df_week.iloc[game_i][\"season\"]\n\n            # Get the ratings for each team\n            home_rating = elo_ratings.query('team == @home_team').iloc[0]['elo_rating']\n            away_rating = elo_ratings.query('team == @away_team').iloc[0]['elo_rating']\n\n            # Calculate the expected score\n            expected_home_score = calculate_expected_score(home_rating, away_rating, hfa=False)\n            expected_away_score = 1 - expected_home_score\n\n            # Calculate the new ratings\n            new_home_rating = calculate_new_rating(home_rating, home_score, expected_home_score)\n            new_away_rating = calculate_new_rating(away_rating, 1 - home_score, expected_away_score)\n\n            # Update the ratings in the DataFrame for the current game\n            elo_ratings.loc[elo_ratings['team'] == home_team, 'elo_rating'] = new_home_rating\n            elo_ratings.loc[elo_ratings['team'] == away_team, 'elo_rating'] = new_away_rating\n\n            # Update the week and season for the teams\n            elo_ratings.loc[elo_ratings['team'] == home_team, 'season'] = game_season\n            elo_ratings.loc[elo_ratings['team'] == home_team, 'week'] = game_week\n            elo_ratings.loc[elo_ratings['team'] == away_team, 'season'] = game_season\n            elo_ratings.loc[elo_ratings['team'] == away_team, 'week'] = game_week\n\n            # to add the values to the original df\n            df.loc[(df['season'] == season) & (df['week'] == week) & (df['home_team'] == home_team), 'home_elo_rating'] = elo_ratings.query('team == @home_team')['elo_rating'].values[0]\n            df.loc[(df['season'] == season) & (df['week'] == week) & (df['away_team'] == away_team), 'away_elo_rating'] = elo_ratings.query('team == @away_team')['elo_rating'].values[0]\n\n        elo_ratings_dict[(season, week)] = elo_ratings.copy()\n    \n    initial_elo_ratings = elo_ratings.copy()\nThen calculating the accuracy.\ndf = df.assign(\n    elo_pred = lambda x: calculate_expected_score(x['away_elo_rating'], x['home_elo_rating'], hfa=False),\n    elo_pred_team = lambda x: np.where(x['elo_pred'] &gt; 0.5, x['home_team'], x['away_team']),\n    winning_team = lambda x: np.where(x['home_win'] == 1, x['home_team'], x['away_team'])\n)\n\ny = df['winning_team']\ny_hat = df['elo_pred_team']\n\nprint(f'Elo Accuracy: {accuracy_score(y, y_hat) * 100:.2f}%')\n\n\n[1] \"Elo Accuracy: 75.38%\"\n\n\nThen visualise the accuracy by season.\naccuracy_df = (\n    df\n    .groupby('season')\n    .apply(lambda x: accuracy_score(x['winning_team'], x['elo_pred_team']) * 100)\n    .reset_index()\n    .rename(columns = {0: 'accuracy'})\n)\n\nyears = range(1999, 2022, 1)\nyear_lst = [datetime(year, 1, 1).strftime(\"'%y\") for year in years]\n\nplt.figure(figsize = (8, 4))\n\nplt.plot(accuracy_df['season'], accuracy_df['accuracy'], marker = 'o', color = '#2ca25f')\nplt.axhline(accuracy_df['accuracy'].mean(), color = 'black', linestyle = '--')\nplt.xticks(years, year_lst)\nplt.title('\\n Dotted Line = Average Accuracy')\nplt.suptitle('Elo Accuracy by Season')\nplt.xlabel('Season')\nplt.ylabel('Accuracy (%)')\nplt.savefig('elo_accuracy.png', dpi = 300, bbox_inches = 'tight')\nplt.show()\n\n\n\nFigure\n\n\nThis is why I like Elo so much. Only by using the two scores from each game you are able to achieve around \\(75\\%\\) accuracy. Accounting for the home team can bring it close to \\(79\\%\\) too. While there may be more complex methods that produce better results, I like to use Elo as an example that sometimes a simple model can achieve fairly good results."
  },
  {
    "objectID": "posts/code_snippets/post.html#rstudio",
    "href": "posts/code_snippets/post.html#rstudio",
    "title": "Code Snippets",
    "section": "RStudio",
    "text": "RStudio\nThis is more specific to the RStudio IDE, but if you want to increase the amount of columns, e.g. more than 50 in View(), you could do:\nrstudioapi::writeRStudioPreference(\"data_viewer_max_columns\", 1000L)\nThere is no limit to the number here."
  },
  {
    "objectID": "posts/code_snippets/post.html#r-1",
    "href": "posts/code_snippets/post.html#r-1",
    "title": "Code Snippets",
    "section": "R",
    "text": "R\nSay you want a dataframe as a list, you could simply do\nmtcars |&gt;\n  purrr::pmap(~ c(...))\nIf each element of the output is a dataframe, then map_dfr is needed to to row-bind them together:\n\n\nNote because we are using .$ we have to use the %&gt;% pipe. R 4.2 allows to do _$ instead of .$, however I use the magrittr here for consistency in the code.\nmtcars %&gt;%\n  split(.$cyl) %&gt;%\n  map(~ lm(mpg ~ wt, data = .x)) %&gt;%\n  map_dfr(~ as.data.frame(t(as.matrix(coef(.)))))\nSay we want to get the correlation for multiple models grouped by a particular variable, it is simple enough\nmtcars %&gt;%\n  split(.$cyl) %&gt;%\n  map(~ lm(mpg ~ wt, data = .x)) %&gt;%\n  map(summary) %&gt;%\n  map_dbl(\"r.squared\")\nTo make predictions, it will be easier to assign a global variable first, e.g.\n# Split into pieces, fit model to each piece, then predict\nby_cyl &lt;- mtcars %&gt;% \n  split(.$cyl)\n\nmods &lt;- by_cyl %&gt;% \n  map(~ lm(mpg ~ wt, data = .))\n  \nmap2(mods, by_cyl, predict)\nSay we want to get a correlation matrix, we can easily do this using the corrr package.\n\ncorrr’s correlatebase cor\n\n\nlibrary(corrr)\n\nmtcars |&gt;\n    dplyr::select(where(is.numeric)) |&gt;\n    dplyr::group_by(cyl) |&gt;\n    dplyr::group_map(~ correlate(.x))\n\n\nmtcars |&gt;\n  dplyr::select(where(is.numeric)) |&gt;\n  dplyr::group_by(cyl) |&gt;\n  dplyr::group_map(~ cor(.x))\n\n\n\nIf we want a long version of each correlation by a group, it is fairly simple too.\nmtcars |&gt;\n    group_by(cyl) |&gt;\n    nest() |&gt;\n    mutate(data = map(data, purrr::compose(stretch, correlate))) |&gt;\n    unnest(cols = c(data))\n\nLM\nMultiple models by a variable\nmodels &lt;- mtcars %&gt;%\n  split(.$cyl) %&gt;%\n  map(~ lm(mpg ~ wt, data = .))\n  \n# extracting a feature\nmodels %&gt;%\n  map(summary) %&gt;%\n  map_dbl(~ .$r.squared)\n  \n# or map_dbl(\"r.squared\")\n\n\nAcross Functions\nAcross functions are quite helpful for data wrangling. Here are some of my most used ones.\nmtcars |&gt;\n    summarise(across(c(mpg:cyl, vs:carb), n_distinct))\n    \nmtcars |&gt;\n  summarise(across(everything(), n_distinct))\n  \nmtcars |&gt;\n  summarise(across(contains(\"r\"), n_distinct))\n  \nmtcars |&gt;\n  summarise(across(where(is.numeric), n_distinct))\n  \nmtcars |&gt;\n  summarise(across(everything(), ~sum(is.na(.))))\nLastly, say you want to replace NA values with 0. You could do this in 2 ways\n\nCustom FunctionAlternative\n\n\n# write a function and then use across with that function\n\n# if the value is NA, replace with 0, else keep it like it is\nreplace0 &lt;- function(x) {\n  ifelse(condition = is.na(x), \n          true = 0, \n          false = as.numeric(x))\n}\n\ndf |&gt;\n  mutate(across(where(is.numeric), replace0))\n\n\ndf %&gt;%\n  mutate(across(where(is.numeric), ~ifelse(is.na(.), 0, as.numeric(.))))"
  }
]